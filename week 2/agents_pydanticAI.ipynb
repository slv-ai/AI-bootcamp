{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f215b8e4-b407-49fd-8d77-13902e06d129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m197 packages\u001b[0m \u001b[2min 2.53s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m63 packages\u001b[0m \u001b[2min 1.05s\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/63] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m63 packages\u001b[0m \u001b[2min 809ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mag-ui-protocol\u001b[0m\u001b[2m==0.1.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manthropic\u001b[0m\u001b[2m==0.71.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1margcomplete\u001b[0m\u001b[2m==3.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.40.57\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.40.57\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcohere\u001b[0m\u001b[2m==5.19.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1meval-type-backport\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastavro\u001b[0m\u001b[2m==1.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgenai-prices\u001b[0m\u001b[2m==0.0.35\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.41.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.46.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.71.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgroq\u001b[0m\u001b[2m==0.33.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.10\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.35.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1minvoke\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogfire\u001b[0m\u001b[2m==4.14.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogfire-api\u001b[0m\u001b[2m==4.14.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistralai\u001b[0m\u001b[2m==1.9.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnexus-rpc\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-instrumentation\u001b[0m\u001b[2m==0.59b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-instrumentation-httpx\u001b[0m\u001b[2m==0.59b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.59b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-util-http\u001b[0m\u001b[2m==0.59b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-ai\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-ai-slim\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-evals\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-graph\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyperclip\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1ms3transfer\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtemporalio\u001b[0m\u001b[2m==1.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtypes-protobuf\u001b[0m\u001b[2m==6.32.1.20250918\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwrapt\u001b[0m\u001b[2m==1.17.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.23.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add pydantic-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d37622-3d7e-42f8-8454-9c2b7241e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ce2654-194e-4adf-91e9-0a719b9ffc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x78e1d872c110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e68d03-d2ea-4aba-b1b3-21c05fd9576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "    \"\"\"Represents a single search result entry.\"\"\"\n",
    "    start: int\n",
    "    content: str\n",
    "    title: str\n",
    "    description: str\n",
    "    filename: str\n",
    "\n",
    "def search(query: str) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "            - start (int): The starting position or offset within the source file.\n",
    "            - content (str): A text excerpt or snippet containing the match.\n",
    "            - title (str): The title of the matched document.\n",
    "            - description (str): A short description of the document.\n",
    "            - filename (str): The path or name of the source file.\n",
    "    \"\"\"\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de492877-e054-4fb4-8420-dc5591f19901",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {}\n",
    "\n",
    "for item in parsed_data:\n",
    "    filename = item['filename']\n",
    "    content = item['content']\n",
    "    file_index[filename] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e7584e7-eb00-4b34-b874-264c6cdd91cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efff8227-a446-435a-bb01-3c0f2fee71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the contents of a file from the file index if it exists.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file to read.\n",
    "\n",
    "    Returns:\n",
    "        str: The file's contents if found, otherwise an error message \n",
    "        indicating that the file does not exist.\n",
    "    \"\"\"\n",
    "    if filename in file_index:\n",
    "        return file_index[filename]\n",
    "    return \"File doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b914f388-85e7-4f1f-8595-8af1d5e7f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd3f2242-3639-4b1f-a2ca-d37d62aa2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation_agent_instructions = \"\"\"\n",
    "You are an assistant that helps improve and generate high-quality documentation for the project.\n",
    "\n",
    "You have access to the following tools:\n",
    "- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\n",
    "- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\n",
    "\n",
    "If `read_file` cannot be used or the file content is unavailable, clearly state:\n",
    "> \"Unable to verify with read_file.\"\n",
    "\n",
    "When answering a question:\n",
    "1. Provide file references for all source materials.  \n",
    "   Use this format:  \n",
    "   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\n",
    "2. If the topic is covered in multiple documents, cite all relevant sources.\n",
    "3. Include code examples whenever they clarify or demonstrate the concept.\n",
    "4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\n",
    "5. If documentation is missing or unclear, infer from context and note that explicitly.\n",
    "\n",
    "Example Citation:\n",
    "See the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca9cef68-8a26-4206-98ae-907e33055d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation_agent = Agent(\n",
    "    name='documentation_agent',\n",
    "    instructions=documentation_agent_instructions,\n",
    "    tools=[search, read_file],\n",
    "    model='openai:gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2845c846-6e2c-4004-bc2b-d0a2e9428cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= await documentation_agent.run(\n",
    "    user_prompt='how do i run llm as a judge evals',\n",
    "    message_history = results.all_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62ce823e-e623-4724-83b8-715d8b92b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run a Large Language Model (LLM) as a judge for evaluations, you can follow the tutorial that outlines a practical example using Python. Here's a brief overview of the steps involved:\n",
      "\n",
      "### Tutorial Overview\n",
      "1. **Install Required Libraries**:\n",
      "   Install the Evidently library which is used to run evaluations.\n",
      "   ```bash\n",
      "   pip install evidently\n",
      "   ```\n",
      "\n",
      "2. **Import Modules and Set Up API**:\n",
      "   You will need to import necessary libraries and set your OpenAI API key.\n",
      "   ```python\n",
      "   import os\n",
      "   os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "   ```\n",
      "\n",
      "3. **Create an Evaluation Dataset**:\n",
      "   Prepare a dataset for evaluation which includes:\n",
      "   - Questions\n",
      "   - Target responses (approved answers)\n",
      "   - New responses (what the system generates)\n",
      "   - Manual labels for evaluation\n",
      "   \n",
      "   This can be done using pandas to create a DataFrame.\n",
      "\n",
      "4. **Design the LLM Evaluator**:\n",
      "   Create and run an LLM evaluator prompt. The LLM will evaluate the responses against the target responses.\n",
      "   ```python\n",
      "   from evidently import Dataset\n",
      "   from evidently import Report\n",
      "   from evidently.presets import TextEvals\n",
      "\n",
      "   report = Report([TextEvals()])\n",
      "   my_eval = report.run(eval_dataset, None)\n",
      "   ```\n",
      "\n",
      "5. **Evaluate the Judge**:\n",
      "   Compare the evaluations made by the LLM with manual labels to assess accuracy. Use classification metrics such as precision, recall, and F1-score to measure the performance of your LLM evaluator.\n",
      "\n",
      "6. **Integrate with Evidently Cloud**:\n",
      "   Optionally, you can upload your evaluation results to Evidently Cloud for easier exploration and tracking.\n",
      "\n",
      "### Simplified Example Code\n",
      "Here's a snippet illustrating the main steps:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "from evidently import Dataset, Report\n",
      "from evidently.presets import TextEvals\n",
      "\n",
      "# Set your OpenAI API key\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "\n",
      "# Create your evaluation dataset\n",
      "data = {\n",
      "    \"Questions\": [\"What is AI?\", \"Explain machine learning.\"],\n",
      "    \"Target Responses\": [\"AI is artificial intelligence.\", \"Machine learning is a subset of AI.\"],\n",
      "    \"New Responses\": [\"AI refers to computers that mimic human intelligence.\", \"ML is part of AI.\"],\n",
      "    \"Manual Labels\": [\"correct\", \"correct\"]\n",
      "}\n",
      "eval_dataset = pd.DataFrame(data)\n",
      "\n",
      "# Run the LLM as a judge\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(Dataset.from_pandas(eval_dataset), None)\n",
      "print(my_eval)\n",
      "```\n",
      "\n",
      "### Additional Resources\n",
      "- An extended video tutorial is available for further insights: [YouTube Tutorial](https://www.youtube.com/watch?v=kP_aaFnXLmY)\n",
      "- For comprehensive details, refer to the documentation on [LLM judges](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx).\n",
      "\n",
      "This tutorial offers a structured approach to implementing an LLM as a judge for various evaluation tasks, allowing for both reference-based and open-ended evaluations.\n"
     ]
    }
   ],
   "source": [
    "print(results.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a258372-be2b-43b1-b541-713de96c3a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method AgentRunResult.new_messages of AgentRunResult(output='To run a Large Language Model (LLM) as a judge for evaluations, you can follow the tutorial that outlines a practical example using Python. Here\\'s a brief overview of the steps involved:\\n\\n### Tutorial Overview\\n1. **Install Required Libraries**:\\n   Install the Evidently library which is used to run evaluations.\\n   ```bash\\n   pip install evidently\\n   ```\\n\\n2. **Import Modules and Set Up API**:\\n   You will need to import necessary libraries and set your OpenAI API key.\\n   ```python\\n   import os\\n   os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n   ```\\n\\n3. **Create an Evaluation Dataset**:\\n   Prepare a dataset for evaluation which includes:\\n   - Questions\\n   - Target responses (approved answers)\\n   - New responses (what the system generates)\\n   - Manual labels for evaluation\\n   \\n   This can be done using pandas to create a DataFrame.\\n\\n4. **Design the LLM Evaluator**:\\n   Create and run an LLM evaluator prompt. The LLM will evaluate the responses against the target responses.\\n   ```python\\n   from evidently import Dataset\\n   from evidently import Report\\n   from evidently.presets import TextEvals\\n\\n   report = Report([TextEvals()])\\n   my_eval = report.run(eval_dataset, None)\\n   ```\\n\\n5. **Evaluate the Judge**:\\n   Compare the evaluations made by the LLM with manual labels to assess accuracy. Use classification metrics such as precision, recall, and F1-score to measure the performance of your LLM evaluator.\\n\\n6. **Integrate with Evidently Cloud**:\\n   Optionally, you can upload your evaluation results to Evidently Cloud for easier exploration and tracking.\\n\\n### Simplified Example Code\\nHere\\'s a snippet illustrating the main steps:\\n\\n```python\\nimport pandas as pd\\nimport os\\nfrom evidently import Dataset, Report\\nfrom evidently.presets import TextEvals\\n\\n# Set your OpenAI API key\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n\\n# Create your evaluation dataset\\ndata = {\\n    \"Questions\": [\"What is AI?\", \"Explain machine learning.\"],\\n    \"Target Responses\": [\"AI is artificial intelligence.\", \"Machine learning is a subset of AI.\"],\\n    \"New Responses\": [\"AI refers to computers that mimic human intelligence.\", \"ML is part of AI.\"],\\n    \"Manual Labels\": [\"correct\", \"correct\"]\\n}\\neval_dataset = pd.DataFrame(data)\\n\\n# Run the LLM as a judge\\nreport = Report([TextEvals()])\\nmy_eval = report.run(Dataset.from_pandas(eval_dataset), None)\\nprint(my_eval)\\n```\\n\\n### Additional Resources\\n- An extended video tutorial is available for further insights: [YouTube Tutorial](https://www.youtube.com/watch?v=kP_aaFnXLmY)\\n- For comprehensive details, refer to the documentation on [LLM judges](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx).\\n\\nThis tutorial offers a structured approach to implementing an LLM as a judge for various evaluation tasks, allowing for both reference-based and open-ended evaluations.')>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.new_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4788f975-1941-4639-863a-f493d0e50d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "user-prompt\n",
      "\n",
      "response\n",
      "tool-call\n",
      "\n",
      "request\n",
      "tool-return\n",
      "\n",
      "response\n",
      "text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in results.new_messages():\n",
    "    print(message.kind)\n",
    "    for part in message.parts:\n",
    "        print(part.part_kind)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a23c0cf1-4177-4338-a705-89ce43fff510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import PydanticAIRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f89400b9-0fae-45b0-a01f-e7fb3b5161fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = PydanticAIRunner(\n",
    "    chat_interface=IPythonChatInterface(),\n",
    "    agent=documentation_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a76137-f6b3-402b-8b26-0c0c27b376e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: how do i run llm as a judge\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"run LLM as a judge\\\"}\")</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"run LLM as a judge\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'start': 0, 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 1000, 'content': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 2000, 'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 20000, 'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 17000, 'content': '\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s ho', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To run a Large Language Model (LLM) as a judge for evaluating responses, you can follow the outlined steps in the tutorial available in the <a href=\"https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx\">LLM as a judge</a>. Below are the essential steps summarized for you:</p>\n",
       "<h3>Prerequisites</h3>\n",
       "<ul>\n",
       "<li>Basic knowledge of Python.</li>\n",
       "<li>An OpenAI API key.</li>\n",
       "</ul>\n",
       "<h3>Steps to Implement LLM as a Judge</h3>\n",
       "<ol>\n",
       "<li><p><strong>Installation</strong>:\n",
       "Install the Evidently library:</p>\n",
       "<pre><code class=\"language-bash\">pip install evidently\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Import Libraries</strong>:\n",
       "Import the required modules in your Python script or Jupyter Notebook:</p>\n",
       "<pre><code class=\"language-python\">import pandas as pd\n",
       "import numpy as np\n",
       "from evidently import Dataset, DataDefinition, Report, BinaryClassification\n",
       "from evidently.presets import TextEvals, ClassificationPreset\n",
       "from evidently.metrics import *\n",
       "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Set Up API Key</strong>:\n",
       "Pass your OpenAI key as an environment variable:</p>\n",
       "<pre><code class=\"language-python\">import os\n",
       "os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;YOUR_KEY&quot;\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create an Evaluation Dataset</strong>:\n",
       "Construct a toy dataset that includes:</p>\n",
       "<ul>\n",
       "<li>Questions (inputs sent to the LLM),</li>\n",
       "<li>Target responses (approved responses),</li>\n",
       "<li>New responses from the system,</li>\n",
       "<li>Manual labels that indicate whether the response is correct.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Create and Run the LLM as a Judge</strong>:\n",
       "Design an evaluation prompt using the <code>BinaryClassificationPromptTemplate</code> to assess the correctness of new responses against the target responses.</p>\n",
       "</li>\n",
       "<li><p><strong>Evaluate the Judge</strong>:\n",
       "Compare the evaluations from the LLM judge with manual labels:</p>\n",
       "<pre><code class=\"language-python\">report = Report([TextEvals()])\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "print(my_eval)\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Assess Quality of LLM Evaluations</strong>:\n",
       "You can evaluate the accuracy and recall of the LLM judge's responses by mapping the dataset's structure accordingly, then running a classification report:</p>\n",
       "<pre><code class=\"language-python\">report = Report([ClassificationPreset()])\n",
       "my_eval = report.run(class_dataset, None)\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Further Steps</strong>:\n",
       "You can iterate on the judge's prompts and explore different evaluator LLMs to enhance performance.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Example Code Snippet</h3>\n",
       "<p>Here’s an example of generating a simple dataset for evaluation:</p>\n",
       "<pre><code class=\"language-python\"># Generate a toy dataset\n",
       "data = {\n",
       "    &quot;questions&quot;: [&quot;What is the capital of France?&quot;, &quot;What is 2 + 2?&quot;],\n",
       "    &quot;target_responses&quot;: [&quot;Paris&quot;, &quot;4&quot;],\n",
       "    &quot;new_responses&quot;: [&quot;Paris&quot;, &quot;four&quot;],\n",
       "    &quot;manual_labels&quot;: [&quot;correct&quot;, &quot;incorrect&quot;]\n",
       "}\n",
       "\n",
       "eval_dataset = pd.DataFrame(data)\n",
       "</code></pre>\n",
       "<p>For more detailed guidance and code examples, refer to the complete tutorial <a href=\"https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx\">here</a>.</p>\n",
       "<p>Implementing an LLM as a judge involves both evaluating the responses and also the effectiveness of the LLM itself, which can be crucial for assessing quality in AI-generated responses.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
