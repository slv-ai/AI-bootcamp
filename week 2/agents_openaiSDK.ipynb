{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12dc1b36-18ab-4e11-980b-f4d6fab4f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m135 packages\u001b[0m \u001b[2min 2.64s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m13 packages\u001b[0m \u001b[2min 168ms\u001b[0m\u001b[0m                                            \n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/13] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m13 packages\u001b[0m \u001b[2min 102ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgriffe\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmcp\u001b[0m\u001b[2m==1.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai-agents\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.48.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtypes-requests\u001b[0m\u001b[2m==2.32.4.20250913\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.38.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b588f583-64e4-4a23-9748-a60ed5a46b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75942708-2b0e-4647-96f1-05e8054b0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent,function_tool,Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7c11e0-e835-4bec-8155-6fc1f1884fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a2c1f3-53fb-4a46-b7a2-6c0696c5941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url):\n",
    "    jina_reader_base_url = 'https://r.jina.ai/'\n",
    "    jina_reader_url = jina_reader_base_url + url\n",
    "    response = requests.get(jina_reader_url)\n",
    "    return response.content.decode('utf-8') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae3f093-41bb-4c71-abbf-3182d984eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import RequestException, HTTPError, Timeout, ConnectionError\n",
    "\n",
    "def fetch_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the content of a URL using the Jina Reader proxy service.\n",
    "\n",
    "    Args:\n",
    "        url (str): The original URL to fetch content from.\n",
    "\n",
    "    Returns:\n",
    "        str: The decoded content of the response.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the URL is invalid or the request fails.\n",
    "    \"\"\"\n",
    "    jina_reader_base_url = 'https://r.jina.ai/'\n",
    "    jina_reader_url = jina_reader_base_url + url\n",
    "\n",
    "    try:\n",
    "        response = requests.get(jina_reader_url, timeout=10)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad status codes (4xx or 5xx)\n",
    "        return response.content.decode('utf-8')\n",
    "    except RequestException as e:\n",
    "        # Catch all network-related errors (e.g., ConnectionError, Timeout, HTTPError)\n",
    "        print(f\"Error fetching URL '{jina_reader_url}': {e}\")\n",
    "        return None\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error decoding response from '{jina_reader_url}'.\")\n",
    "        return None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eac7826-1f22-481b-90f9-a42342e37daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = fetch_url('https://datatalks.club')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4215ce4e-987c-4024-b21b-e545d01a3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_agent = Agent(\n",
    "    name = 'web-agent',\n",
    "    instructions=\"you're a helful assistnat\",\n",
    "    model='gpt-4o-mini',\n",
    "    tools=[function_tool(fetch_url)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc45153-ffdd-4897-b6d2-e5bf9aad990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner()\n",
    "question = \"what is this page about? https://openai.github.io/openai-agents-python/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d59466e1-49e2-4906-a5ce-7393667e4f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10707/3773915482.py:1: RuntimeWarning: coroutine 'Runner.run' was never awaited\n",
      "  result = await runner.run(web_agent, input = question)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "result = await runner.run(web_agent, input = question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3be858d-d31e-4edb-92c1-cf14803fb2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page you referred to is about the **OpenAI Agents SDK**, which is designed to facilitate the development of agentic AI applications using a straightforward, user-friendly package. Here are the key points:\n",
      "\n",
      "### Overview\n",
      "- The **OpenAI Agents SDK** allows developers to build applications with AI agents that utilize language models (LLMs).\n",
      "- It features primitives like **Agents**, **Handoffs**, **Guardrails**, and **Sessions** to manage and maintain conversation histories and delegate tasks.\n",
      "\n",
      "### Main Features\n",
      "1. **Agent Loop**: Automatically manages interactions and responses between the agent and tools.\n",
      "2. **Python-first**: Leverages Python's built-in capabilities for easy orchestration without needing new abstractions.\n",
      "3. **Handoffs**: Enables agents to delegate specific tasks to other agents.\n",
      "4. **Guardrails**: Provides validation for inputs and outputs to ensure accuracy.\n",
      "5. **Sessions**: Manages conversation history without manual intervention.\n",
      "6. **Function Tools**: Converts Python functions into tools with automatic schema generation.\n",
      "7. **Tracing**: Built-in visualization and debugging tools to monitor agent operations.\n",
      "\n",
      "### Installation\n",
      "The SDK can be installed using a simple pip command:\n",
      "```bash\n",
      "pip install openai-agents\n",
      "```\n",
      "\n",
      "### Example Usage\n",
      "A basic example demonstrates creating an agent to write a haiku about recursion:\n",
      "```python\n",
      "from agents import Agent, Runner\n",
      "\n",
      "agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n",
      "\n",
      "result = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\n",
      "print(result.final_output)\n",
      "```\n",
      "\n",
      "### Additional Resources\n",
      "The page includes documentation for installation, quickstart guides, usage examples, and a comprehensive API reference. It aims to be both powerful and accessible for developers looking to create advanced AI applications.\n",
      "\n",
      "For more detailed information, you can explore the full [OpenAI Agents SDK documentation](https://openai.github.io/openai-agents-python/).\n"
     ]
    }
   ],
   "source": [
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d22c95a8-6ff1-4646-a5a3-669161d849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = result.new_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3089829c-6138-4df6-b30a-6a06cf87956c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolCallItem(agent=Agent(name='web-agent', handoff_description=None, tools=[FunctionTool(name='fetch_url', description='Fetches the content of a URL using the Jina Reader proxy service.', params_json_schema={'properties': {'url': {'description': 'The original URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x745c7b68c680>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions=\"you're a helful assistnat\", prompt=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{\"url\":\"https://openai.github.io/openai-agents-python/\"}', call_id='call_l9osn32NF4Cap5YUGpV7lZR1', name='fetch_url', type='function_call', id='fc_0665c6d8cc8182d40068f8ec9ba3d88198a9b90153aa885045', status='completed'), type='tool_call_item')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "948730a4-0694-4356-a8c5-627ba8b93009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFunctionToolCall(arguments='{\"url\":\"https://openai.github.io/openai-agents-python/\"}', call_id='call_l9osn32NF4Cap5YUGpV7lZR1', name='fetch_url', type='function_call', id='fc_0665c6d8cc8182d40068f8ec9ba3d88198a9b90153aa885045', status='completed')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0].raw_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f959cb-64a0-4826-b763-b698cd4d6343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The page you referred to is about the **OpenAI Agents SDK**, which is designed to facilitate the development of agentic AI applications using a straightforward, user-friendly package. Here are the key points:\\n\\n### Overview\\n- The **OpenAI Agents SDK** allows developers to build applications with AI agents that utilize language models (LLMs).\\n- It features primitives like **Agents**, **Handoffs**, **Guardrails**, and **Sessions** to manage and maintain conversation histories and delegate tasks.\\n\\n### Main Features\\n1. **Agent Loop**: Automatically manages interactions and responses between the agent and tools.\\n2. **Python-first**: Leverages Python\\'s built-in capabilities for easy orchestration without needing new abstractions.\\n3. **Handoffs**: Enables agents to delegate specific tasks to other agents.\\n4. **Guardrails**: Provides validation for inputs and outputs to ensure accuracy.\\n5. **Sessions**: Manages conversation history without manual intervention.\\n6. **Function Tools**: Converts Python functions into tools with automatic schema generation.\\n7. **Tracing**: Built-in visualization and debugging tools to monitor agent operations.\\n\\n### Installation\\nThe SDK can be installed using a simple pip command:\\n```bash\\npip install openai-agents\\n```\\n\\n### Example Usage\\nA basic example demonstrates creating an agent to write a haiku about recursion:\\n```python\\nfrom agents import Agent, Runner\\n\\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\\n\\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\\nprint(result.final_output)\\n```\\n\\n### Additional Resources\\nThe page includes documentation for installation, quickstart guides, usage examples, and a comprehensive API reference. It aims to be both powerful and accessible for developers looking to create advanced AI applications.\\n\\nFor more detailed information, you can explore the full [OpenAI Agents SDK documentation](https://openai.github.io/openai-agents-python/).'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[-1].raw_item.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88c13fd6-b772-430a-ab87-528f62544005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIAgentsSDKRunner\n",
    "\n",
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIAgentsSDKRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=web_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f9ede10-3fae-4e8f-b2b7-25706b650e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: what are openai agents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>OpenAI agents refer to AI systems developed by OpenAI that can autonomously perform tasks, make decisions, or interact with humans in various contexts. These agents leverage advanced language models, like GPT, to understand natural language, respond to queries, generate content, or facilitate conversations.</p>\n",
       "<h3>Key Features of OpenAI Agents:</h3>\n",
       "<ol>\n",
       "<li><p><strong>Natural Language Understanding</strong>: They excel at interpreting and processing human language, making them suitable for a wide range of applications.</p>\n",
       "</li>\n",
       "<li><p><strong>Task Automation</strong>: Agents can automate repetitive tasks, improving efficiency in various sectors like customer service, data entry, and content generation.</p>\n",
       "</li>\n",
       "<li><p><strong>Interactive</strong>: Many agents are designed to engage in dialogue, allowing for dynamic interactions that can be personalized to user needs.</p>\n",
       "</li>\n",
       "<li><p><strong>Learning Capability</strong>: They can learn from user interactions and adapt their responses over time to better serve users' needs.</p>\n",
       "</li>\n",
       "<li><p><strong>Integration</strong>: OpenAI agents can be integrated into applications, websites, and other platforms to enhance functionality and user experience.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Applications:</h3>\n",
       "<ul>\n",
       "<li><strong>Customer Support</strong>: Providing instant responses and assistance.</li>\n",
       "<li><strong>Content Creation</strong>: Generating articles, reports, and social media posts.</li>\n",
       "<li><strong>Data Analysis</strong>: Interpreting datasets and summarizing findings.</li>\n",
       "<li><strong>Education</strong>: Tutoring students and providing personalized learning experiences.</li>\n",
       "</ul>\n",
       "<p>These agents are part of a broader movement towards creating intelligent systems that can assist humans in various domains.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "950199b3-5f20-47ed-acce-0045715ad370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def format_timestamp(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to H:MM:SS if > 1 hour, else M:SS\"\"\"\n",
    "    total_seconds = int(seconds)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, secs = divmod(remainder, 60)\n",
    "\n",
    "    if hours > 0:\n",
    "        return f\"{hours}:{minutes:02}:{secs:02}\"\n",
    "    else:\n",
    "        return f\"{minutes}:{secs:02}\"\n",
    "\n",
    "\n",
    "def make_subtitles(transcript) -> str:\n",
    "    lines = []\n",
    "\n",
    "    for entry in transcript:\n",
    "        ts = format_timestamp(entry.start)\n",
    "        text = entry.text.replace('\\n', ' ')\n",
    "        lines.append(ts + ' ' + text)\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "    \n",
    "def fetch_transcript_raw(video_id):\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "    transcript = ytt_api.fetch(video_id)\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def fetch_transcript_text(video_id):\n",
    "    transcript = fetch_transcript_raw(video_id)\n",
    "    subtitles = make_subtitles(transcript)\n",
    "    return subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3acec77-29c2-4b98-ae1c-bcfb5b661545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def fetch_transcript_cached(video_id):\n",
    "    cache_dir = Path(\"../data_cache/youtube_videos\")\n",
    "    print(cache_dir)\n",
    "    cache_file = cache_dir / f\"{video_id}.txt\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        return cache_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    subtitles = fetch_transcript_text(video_id)\n",
    "    cache_file.write_text(subtitles, encoding=\"utf-8\")\n",
    "\n",
    "    return subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d6f4622-03f2-407d-b2bd-c4eb036eb395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_cache/youtube_videos\n",
      "0:00 First of all, thank you very much for uh\n",
      "0:02 for joining the event. This is the first\n",
      "0:04 event we ever have. Uh so this is a new\n",
      "0:07 experience for us as well. So I hope\n",
      "0:10 everything um goes smoothly\n",
      "0:13 and this is the first event but not the\n",
      "0:16 last one. We already have uh a few\n",
      "0:18 events planned. So the first one, this\n",
      "0:20 one is about deploying models with\n",
      "0:22 SageMaker. But u we want to meet every\n",
      "0:26 Tuesday\n",
      "0:27 at u 5:00 p.m. uh European time to talk\n",
      "0:31 about differ\n"
     ]
    }
   ],
   "source": [
    "subtitles = fetch_transcript_cached('2ZOnA19sDpM')\n",
    "print(subtitles[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e01495aa-9b55-4030-9a9b-7921dd3654fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_instructions = \"\"\"\n",
    "You're a helpful assistant that helps answer user questions\n",
    "about YouTube videos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1da19723-ed8b-4466-a24e-d47f6aba5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_youtube_transcript(video_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the transcript of a YouTube video and converts it into a subtitle-formatted string.\n",
    "\n",
    "    Args:\n",
    "        video_id (str): The unique YouTube video ID.\n",
    "\n",
    "    Returns:\n",
    "        str: The subtitles generated from the video's transcript.\n",
    "    \"\"\"\n",
    "    return fetch_transcript_cached(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ba3e83a-c9b7-4ece-b8de-88282afbb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_agent = Agent(\n",
    "    name='youtube_agent',\n",
    "    instructions=summary_instructions,\n",
    "    tools=[function_tool(fetch_youtube_transcript)],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ac1e481-13db-48fb-aa9a-2e0a76eff9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = OpenAIAgentsSDKRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=youtube_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cad7b42e-dde7-4282-9d44-a5730b67316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: summarize the video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>I can help with that! Please provide the YouTube video ID or link you'd like summarized.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: 2ZOnA19sDpM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_cache/youtube_videos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>fetch_youtube_transcript({\"video_id\":\"2ZOnA19sDpM\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"video_id\":\"2ZOnA19sDpM\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>0:00 First of all, thank you very much for uh\n",
       "0:02 for joining the event. This is the first\n",
       "0:04 event we ever have. Uh so this is a new\n",
       "0:07 experience for us as well. So I hope\n",
       "0:10 everything um goes smoothly\n",
       "0:13 and this is the first event but not the\n",
       "0:16 last one. We already have uh a few\n",
       "0:18 events planned. So the first one, this\n",
       "0:20 one is about deploying models with\n",
       "0:22 SageMaker. But u we want to meet every\n",
       "0:26 Tuesday\n",
       "0:27 at u 5:00 p.m. uh European time to talk\n",
       "0:31 about different uh technical topics. So\n",
       "0:33 the next week we will have a topic about\n",
       "0:35 customer segmentation. Then we will have\n",
       "0:37 a topic about uh fighting uh fraud uh\n",
       "0:41 with triplet loss about neural nets and\n",
       "0:43 then um slightly different topic more on\n",
       "0:46 the soft side of skills about\n",
       "0:49 communication skills for um data\n",
       "0:52 scientists and other data professionals.\n",
       "0:56 In addition to that, we also want to\n",
       "0:57 have something uh like a lighter format\n",
       "1:00 without slides, just u QA Q&A um just a\n",
       "1:04 conversation that we want to also put as\n",
       "1:07 a podcast later. Um so we haven't\n",
       "1:11 officially planned one yet but the first\n",
       "1:14 we want to run uh next week on Friday\n",
       "1:17 and we'll talk about roles in a data\n",
       "1:20 team\n",
       "1:22 and um yeah so you if you want to if you\n",
       "1:26 found this uh event elsewhere like on\n",
       "1:29 LinkedIn or somewhere else but you\n",
       "1:31 haven't joined the community yet so this\n",
       "1:33 is the link you can use for joining it's\n",
       "1:35 a slack community\n",
       "1:37 so it's very simple just join join data\n",
       "1:40 talks.cl up and this is this will give\n",
       "1:43 you directly an invite invite link to\n",
       "1:46 Slack\n",
       "1:47 and then finally for today we will use\n",
       "1:49 slider for questions.\n",
       "1:52 Uh let me actually uh just put this to\n",
       "1:55 chat\n",
       "1:57 and if you have any questions during the\n",
       "1:59 presentation just use slider to put your\n",
       "2:01 question there and if you see a question\n",
       "2:04 somebody\n",
       "2:06 asked already a question you wanted to\n",
       "2:07 ask just up it. So I will just share\n",
       "2:11 this link in chat\n",
       "2:15 and then VI can start.\n",
       "2:25 So thank you uh Dimmitri the floor is\n",
       "2:29 yours.\n",
       "2:30 Hi there.\n",
       "2:32 So um thank you all for\n",
       "2:36 um for coming uh to this webinar. So let\n",
       "2:41 me start. I will share my screen then.\n",
       "2:44 Um\n",
       "2:47 so can you please uh indicate in the\n",
       "2:49 chat that you can see the screen?\n",
       "2:54 Everything is good.\n",
       "2:55 Maybe with plus sign or something thumbs\n",
       "2:58 up.\n",
       "3:00 Okay cool.\n",
       "3:20 Okay. So let me start. Um so basically\n",
       "3:26 uh let's start with the uh with our\n",
       "3:30 first topic for our u data talks club.\n",
       "3:34 Um so basically we will discuss today\n",
       "3:36 deployment of uh dockerized ML models in\n",
       "3:39 AWS Sage Maker. So first of all I would\n",
       "3:42 like you once I would like once again to\n",
       "3:45 uh say thank you that uh you have come\n",
       "3:48 and uh like for for showing the interest\n",
       "3:51 in the topic and in the community\n",
       "3:53 itself. Um and uh secondly would like to\n",
       "3:57 um introduce myself. So my name is Mitri\n",
       "3:59 Muzaleki and I'm working as a lead data\n",
       "4:02 scientist in um Audi. So it's okay.\n",
       "4:07 Um so actually we are uh Berlin based uh\n",
       "4:12 company which is uh working on the\n",
       "4:15 fields of um uh helping the people with\n",
       "4:18 the hearing loss. So working with the\n",
       "4:20 hearing aids. So and our um goal uh as a\n",
       "4:25 data team as a basically data science\n",
       "4:27 team is to um help the people with the\n",
       "4:31 hearing loss. Uh so um to have the\n",
       "4:35 better hearing uh device and also like\n",
       "4:38 uh to to be able to identify um these\n",
       "4:42 people yeah who have this uh hearing\n",
       "4:44 loss. Yeah. So uh why I have selected\n",
       "4:49 this topic? So basically uh in Odena we\n",
       "4:52 have the data platform which was hosted\n",
       "4:56 uh which is hosted on AWS. So we have um\n",
       "5:01 a lot of resources there. So when our uh\n",
       "5:03 database is hosted there so our other\n",
       "5:06 resources hosted there and basically all\n",
       "5:07 our stack for the uh data science is\n",
       "5:10 also uh based there. So we are using AWS\n",
       "5:15 SageMaker for training and deployment\n",
       "5:18 deployment of our models. Um therefore\n",
       "5:21 basically this is our let's say standard\n",
       "5:23 platform\n",
       "5:25 uh for the research and for the\n",
       "5:27 deployment of the models. Yeah. And uh\n",
       "5:29 basically the topic that I would like to\n",
       "5:31 discuss today uh we also um you know use\n",
       "5:36 extensively in our work. So we uh create\n",
       "5:40 the dockerized ML models and uh uh push\n",
       "5:45 them to the AWS and then uh deploy using\n",
       "5:49 AWS SageMaker.\n",
       "5:51 So uh what will be the let's say the\n",
       "5:53 topic and the uh let's say the content\n",
       "5:56 of our uh today meeting. So I prepared\n",
       "6:02 the uh small repository for this\n",
       "6:04 meeting. So afterwards you can have the\n",
       "6:06 uh link and uh have an access to that\n",
       "6:09 and uh um also um trying to use that and\n",
       "6:14 play a bit with the data and use it for\n",
       "6:17 your own data for your own projects. Um\n",
       "6:20 so what will be the uh contents of this\n",
       "6:23 meeting? So we will um discuss regarding\n",
       "6:27 the docker itself. So what it is in a\n",
       "6:31 quick words. So how um how men um can\n",
       "6:35 use that and u um so how you can use\n",
       "6:39 that also in the uh AWS hagemaker\n",
       "6:42 um infrastructure. Then we basically\n",
       "6:44 talk we'll talk about the um ML model\n",
       "6:48 structure. So and how basically we can\n",
       "6:50 combine the uh docker itself with the ML\n",
       "6:54 model structure. We talk a bit about um\n",
       "6:57 basically flask application and rest uh\n",
       "7:01 API services that we can basically use\n",
       "7:04 for u building our web models. Uh then\n",
       "7:08 afterwards we will talk a bit about the\n",
       "7:11 execution stack. So how exactly\n",
       "7:14 SageMaker will execute your docker\n",
       "7:16 container with your code. Then a couple\n",
       "7:19 words about ECG. So um regarding the web\n",
       "7:23 server gateway interface. So basically\n",
       "7:25 the interface that will help us uh in\n",
       "7:28 our application to connect our server\n",
       "7:31 side and our model site. So um then a\n",
       "7:36 couple of words about the structure of\n",
       "7:38 this repositories about the main\n",
       "7:40 components and the structure of our uh\n",
       "7:43 container application of our code. Um\n",
       "7:45 basically yeah so um let's start\n",
       "7:51 um let's start with the introduc\n",
       "7:53 introduction of uh AWS SageMaker uh\n",
       "7:56 estimator itself. So uh basically to use\n",
       "8:01 uh AWS uh SageMaker uh estimator you\n",
       "8:05 need to understand that uh basically\n",
       "8:07 this is they use the different approach\n",
       "8:09 for example that skyit learn using yeah\n",
       "8:11 or some other um well-known data science\n",
       "8:14 library. So uh every estimator in\n",
       "8:17 SageMaker even it's um basically uh\n",
       "8:21 official and um um designed by AWS such\n",
       "8:26 as lineer learner org boost um or custom\n",
       "8:31 estimators they are all having the same\n",
       "8:34 structure. So basically each estimator\n",
       "8:36 in Sage Maker this is the dockerized uh\n",
       "8:40 web service. Yeah. So docerized web\n",
       "8:43 service and um basically this web\n",
       "8:46 service provides the rest API\n",
       "8:49 infrastructure and uh you can think of\n",
       "8:52 it as a a combination of uh training and\n",
       "8:57 uh predict scripts. Yeah. Which uh uh\n",
       "9:01 wrapped up in the uh flask application\n",
       "9:04 that is providing this recipe uh\n",
       "9:07 infrastructure.\n",
       "9:09 Um so basically if you uh for some\n",
       "9:13 reason uh would like to use your custom\n",
       "9:17 estimators\n",
       "9:18 um you should be able to know the\n",
       "9:20 structure of how to uh organize this\n",
       "9:23 estimator and how to combine them and\n",
       "9:25 how to orchestrate it with the docker.\n",
       "9:28 Yeah. So therefore for us for example in\n",
       "9:32 audibin it's very important uh because\n",
       "9:33 we have a variety of tasks that we need\n",
       "9:36 to combine the certain algorithms for\n",
       "9:40 example we need to do um not uh uh not\n",
       "9:45 just using for example HG boost for uh\n",
       "9:48 classification but for example combine\n",
       "9:50 the different uh ensembles doing some uh\n",
       "9:53 stacking procedure or uh voting\n",
       "9:56 procedure both for classification or\n",
       "9:58 regression. Therefore, um it's not um\n",
       "10:01 allowed yet. Maybe that would be that\n",
       "10:05 will be the support uh later on in Sage\n",
       "10:08 Maker about that. Uh but right now there\n",
       "10:10 is no support doing some uh voting uh\n",
       "10:14 procedure for example taking the\n",
       "10:16 different ensembles. Um therefore if you\n",
       "10:19 would like to use this uh approach and\n",
       "10:22 this procedure uh more deeper ones yeah\n",
       "10:25 then you need to go to the custom models\n",
       "10:27 and do it um yourself. Yeah. And of\n",
       "10:31 course um basically you uh doing your\n",
       "10:34 custom models you are not strictly uh\n",
       "10:37 stayed with one um library such as skyit\n",
       "10:41 learn you can use variety of the\n",
       "10:43 frameworks. So for example if you work\n",
       "10:45 with the images you can uh easily use\n",
       "10:47 tensorflow or kas or mxnet so there is a\n",
       "10:51 possibility of the ways what frameworks\n",
       "10:53 and for what type of tasks you can use\n",
       "10:57 um so let's start with the docker so\n",
       "11:00 here I put the uh picture um of course I\n",
       "11:05 uh think that there are a lot of people\n",
       "11:06 who and um I hope that there are a lot\n",
       "11:09 of people who already know the docker\n",
       "11:11 but we'll put uh some words about that.\n",
       "11:14 Um so basically Docker provides the um\n",
       "11:19 simplistic way how to uh package your\n",
       "11:21 code. Yeah. So by building the images.\n",
       "11:25 So uh Docker um give you the possibility\n",
       "11:29 to build the image based on so-called\n",
       "11:32 Docker file which is basically the text\n",
       "11:34 document where all of the commands are\n",
       "11:38 located. Yeah. So and this commands are\n",
       "11:40 using by docker during the uh building\n",
       "11:44 creation of the uh image itself. Yeah.\n",
       "11:47 And after creation of this image, this\n",
       "11:50 image can be uh running. Yeah. So and u\n",
       "11:54 running in the docker container.\n",
       "11:57 Um so basically if we're comparing this\n",
       "12:01 one uh this approach the dockerized\n",
       "12:03 approach with the other approach like\n",
       "12:06 using cond or using virtual enth yeah\n",
       "12:09 yeah like some other um virtual n\n",
       "12:13 approaches in python. Yeah. So we can\n",
       "12:14 see that u docker um is like a fully\n",
       "12:19 independent\n",
       "12:20 regarding the langu the language that\n",
       "12:22 you can use and it's also help you to\n",
       "12:25 create the whole um infrastructure the\n",
       "12:28 whole environment with the starting with\n",
       "12:30 the assigning environmental variables\n",
       "12:32 etc and in that way so you can think of\n",
       "12:36 a docker as a basically kind of\n",
       "12:38 lightweight version of uh virtual\n",
       "12:41 machine then you can use for packaging\n",
       "12:44 and then running your codes on the\n",
       "12:47 different services. So as in our case on\n",
       "12:50 AWS\n",
       "12:53 um so let's uh speak a bit regarding the\n",
       "12:58 uh structure of ML Docker for Sage\n",
       "13:02 Maker. So basically uh if you use docker\n",
       "13:07 so you cannot use it like um basically\n",
       "13:10 how you like yeah you should uh combine\n",
       "13:13 this with the uh with the predefined\n",
       "13:15 settings for um docker images in sage\n",
       "13:19 maker. So therefore on this picture you\n",
       "13:22 can see the structure\n",
       "13:25 uh which is using in sage maker which\n",
       "13:27 sagemaker estimator is using when it's\n",
       "13:30 uh working with the docker images. So\n",
       "13:33 here is the\n",
       "13:35 uh main folder called opt ml and\n",
       "13:38 basically it has three main folders\n",
       "13:41 three main channels it's uh input output\n",
       "13:45 and model. So what it is let's discuss\n",
       "13:48 it um let's discuss it uh right now. So\n",
       "13:53 regarding the input so regarding the\n",
       "13:55 input so we have here the information\n",
       "13:58 about the config. So for example here we\n",
       "14:01 can store the information about the\n",
       "14:02 hyperparameters of our ML model for\n",
       "14:05 example in the JSON format and then this\n",
       "14:08 information can be uh downloaded and\n",
       "14:12 using in the uh SageMaker estimator also\n",
       "14:16 we can store here information about the\n",
       "14:18 resource config also for example in JSON\n",
       "14:21 format if it's differs from the standard\n",
       "14:23 variant\n",
       "14:25 um also here you can uh store the\n",
       "14:28 information that is um basically\n",
       "14:33 um that is going to your model. For\n",
       "14:35 example, u the results of the training\n",
       "14:38 when you train your model and receive\n",
       "14:41 the uh basically\n",
       "14:43 the results. Yeah. So, uh fitted\n",
       "14:46 estimator. So and then afterwards you\n",
       "14:49 can uh save the weights for example\n",
       "14:52 using some uh frameworks as a pickle or\n",
       "14:56 job lip um and use this weights after\n",
       "14:59 afterwards\n",
       "15:01 uh for the prediction. So this\n",
       "15:03 information for example can be stored\n",
       "15:04 here uh under uh folder model. Yeah. And\n",
       "15:09 the output is basically the folder the\n",
       "15:12 channel um which is designed for storing\n",
       "15:16 uh such\n",
       "15:18 data as a status data for example the\n",
       "15:20 failure data or the success data of uh\n",
       "15:24 training your algorithm. Yeah. So this\n",
       "15:27 is the major structure which you need to\n",
       "15:30 follow to basically construct your\n",
       "15:33 dockerized model and uh that it will be\n",
       "15:38 um easily run uh with sage maker.\n",
       "15:44 So now a bit regarding the overall\n",
       "15:46 structure regarding the execution stack\n",
       "15:48 for the container. So on this picture so\n",
       "15:52 there is displayed basically the whole\n",
       "15:55 uh procedure of how uh SageMaker\n",
       "15:57 estimator uh is working. Yeah. So\n",
       "16:01 basically uh it all starts with the\n",
       "16:03 client. So the client send the requests.\n",
       "16:06 So it's like usually it's a post\n",
       "16:08 requests in ML models. We're interesting\n",
       "16:10 in post requests sending the uh x\n",
       "16:14 feature vector uh in the body.\n",
       "16:18 um and this request going directly to\n",
       "16:21 the server. In this um configuration we\n",
       "16:24 use in Jinx as a server. So but it's\n",
       "16:28 possible also to use some other\n",
       "16:30 alternatives such as Apache. Yeah. Then\n",
       "16:33 afterwards um using reverse proxy this\n",
       "16:37 information goes to the VCGI. In this\n",
       "16:41 configuration we use unicorn and unicorn\n",
       "16:44 is basically uh used as a load balancer\n",
       "16:48 let's say as a bridge between the uh\n",
       "16:52 server side from one side and between\n",
       "16:54 the application side from another as an\n",
       "16:58 application we using uh flask\n",
       "17:01 application above uh our code. Yeah. And\n",
       "17:05 we uh do it in a way there are like a\n",
       "17:08 couple ways how to do that. So of course\n",
       "17:09 you can do it um inside uh let's say uh\n",
       "17:14 one entity one file um but here we do it\n",
       "17:18 in a way that uh will be easy for\n",
       "17:20 SageMaker to understand. We will u\n",
       "17:23 separate it in the train file for the\n",
       "17:26 training and predict file for the\n",
       "17:28 prediction. Of course, if you would like\n",
       "17:30 to make it more complex,\n",
       "17:32 um let's imagine that you would like to\n",
       "17:34 have the um prep-processing steps in a\n",
       "17:38 different other files or you would like\n",
       "17:40 to have the scripts that would uh\n",
       "17:43 contains of some helper functions. Of\n",
       "17:45 course, you can do that and organize it\n",
       "17:48 in a way of kind of like a modules uh of\n",
       "17:51 um uh kind of production code in Python.\n",
       "17:55 Yeah. So, it's also possible. It's al\n",
       "17:58 depending on your task which you are\n",
       "17:59 solving\n",
       "18:01 um the results after so you um you were\n",
       "18:08 working with the models you receive two\n",
       "18:10 endpoints which you can use basically.\n",
       "18:12 So the first endpoint is called ping. Uh\n",
       "18:16 pink is just a simple uh health check\n",
       "18:19 for your web service which is basically\n",
       "18:22 uh you sending the u get request and uh\n",
       "18:29 if the service is alive and is working\n",
       "18:32 so you receive back the uh success\n",
       "18:36 status uh status code equal uh 200.\n",
       "18:40 Yeah. If there is some problem with your\n",
       "18:42 service. So you receive 404 error code.\n",
       "18:47 Um this is basically uh as in the\n",
       "18:50 majority of uh flask application uh you\n",
       "18:53 have this kind of so-called ping or\n",
       "18:56 health or health check endpoint which\n",
       "18:58 you're using for um basically checking\n",
       "19:02 whether your service is working is\n",
       "19:04 operating or not. And the second\n",
       "19:06 endpoint which is the most interesting\n",
       "19:09 for us which is basically the core of\n",
       "19:12 our work is called invocations. So\n",
       "19:14 invocations is the endpoint that\n",
       "19:17 receives uh post request from the\n",
       "19:19 client. So post request uh contains body\n",
       "19:23 and body contains uh x feature vector\n",
       "19:27 that uh client side sent to the uh to\n",
       "19:31 the model and then basically uh receive\n",
       "19:34 back uh the results of the prediction as\n",
       "19:37 a response either for example\n",
       "19:39 information about the class and the\n",
       "19:42 probability of the class in the\n",
       "19:44 classification tasks or uh basically the\n",
       "19:48 continuous uh variable le um when doing\n",
       "19:52 the regression tasks. Yeah.\n",
       "19:55 Um to basically um summarize this point,\n",
       "20:01 let us check a bit regarding um VCG. So\n",
       "20:06 this is basically u so-called web server\n",
       "20:09 gateway interface and you can think of\n",
       "20:12 it as a bridge as I told. Yeah. So be uh\n",
       "20:15 between the u server side and between\n",
       "20:18 the application side which in our case\n",
       "20:21 would be flask web service. Yeah. Um so\n",
       "20:27 basically it works like that. So the\n",
       "20:29 server executes the web app and uh sends\n",
       "20:33 uh information and a callback function\n",
       "20:35 to the app. Yeah. So this is the start.\n",
       "20:38 Then the request is uh proceeded on the\n",
       "20:42 website and response is sent back uh to\n",
       "20:45 the server. Yeah. Utilizing uh uh\n",
       "20:48 callback function. So this is basically\n",
       "20:51 the basic principle how the uh how the\n",
       "20:54 VCGI is working.\n",
       "20:56 Um so basically example of the\n",
       "20:59 frameworks that is using VCGI um that\n",
       "21:03 supports VCGI there are a lot of them.\n",
       "21:05 Yeah. So for example we already uh\n",
       "21:08 spoken about flask we can also include\n",
       "21:12 um tornado to that the lightweight\n",
       "21:15 framework and also um something else\n",
       "21:18 like for example jungle yeah that is\n",
       "21:21 using for python web development mostly.\n",
       "21:26 Yeah. So uh basically right now uh we\n",
       "21:31 are ready to go to the content itself to\n",
       "21:34 the content of our repositories\n",
       "21:37 and before that let's um briefly discuss\n",
       "21:41 what we will have there and then we will\n",
       "21:43 go deeper and check the code of our\n",
       "21:47 files in the repository. So regarding\n",
       "21:49 the main components uh so the main\n",
       "21:51 components which we have in the root of\n",
       "21:53 our repository. So we have here docker\n",
       "21:56 file which we already mentioned that is\n",
       "21:59 uh text document file that contains all\n",
       "22:01 the required operations uh that docker\n",
       "22:05 use for uh producing the image via\n",
       "22:09 docker build command.\n",
       "22:11 Um then basically we have uh this age\n",
       "22:14 maker estimator folder. So basically\n",
       "22:16 this is our main working directory where\n",
       "22:20 all of our files are stored and we have\n",
       "22:23 also shell file. So this shell file\n",
       "22:26 shell script uh is basically helping us\n",
       "22:31 to create to achieve two uh two aims\n",
       "22:35 yeah two goals. So the first uh we use\n",
       "22:39 this file to take the information about\n",
       "22:42 the account and about the region of our\n",
       "22:45 AWS service that we are using. Yeah. So\n",
       "22:48 that meaning that um before using this\n",
       "22:53 shell script we need to uh install\n",
       "22:55 pre-install AWS clement\n",
       "22:58 line interface in AWS and run comment\n",
       "23:02 AWS configure this comment. uh after you\n",
       "23:05 run it in your terminal after you have\n",
       "23:07 installed uh AWS cle so you will receive\n",
       "23:10 four questions. So the first one will be\n",
       "23:14 uh please provide your public access key\n",
       "23:17 then please provide your private uh\n",
       "23:19 access key then please provide the\n",
       "23:21 region in which you are working uh on\n",
       "23:25 AWS and the fourth would be please\n",
       "23:27 provide the desirable format of the uh\n",
       "23:30 of the output. Yeah. So when you answer\n",
       "23:32 of all of these four questions so you\n",
       "23:34 can basically start using AWS cle and uh\n",
       "23:38 doing some operations let's say uh\n",
       "23:41 creating repositories yeah uh or\n",
       "23:44 creating some buckets on S3 yeah or\n",
       "23:48 listing the uh the um list of the EC2\n",
       "23:53 instances that is available on your\n",
       "23:55 account. This is basically the first\n",
       "23:58 goal which we are achieving using this\n",
       "24:01 uh shell script and um the second goal\n",
       "24:05 is basically the docker goal. So we need\n",
       "24:07 to uh build our docker and this shell\n",
       "24:11 script will actually help us to do that.\n",
       "24:12 So we will uh make we should make a\n",
       "24:15 docker build. Yeah, basically create our\n",
       "24:19 image. Then we need to make a docker\n",
       "24:21 tag. Put the uh tag uh with the\n",
       "24:25 information about the account and region\n",
       "24:28 uh and the name of the image and the tag\n",
       "24:31 latest to the image so that we would\n",
       "24:35 understand um what it is and what is\n",
       "24:38 basically the latest image that\n",
       "24:40 SageMaker need to use and need to\n",
       "24:42 execute. Uh and uh the last step is\n",
       "24:45 basically docker push. So after creating\n",
       "24:48 the uh image after tagging so we are\n",
       "24:51 ready basically to make a docker push\n",
       "24:53 and afterwards uh this image is uh being\n",
       "24:58 pushed to the uh ECR uh repository. ECR\n",
       "25:03 is basically the service on AWS. It's um\n",
       "25:06 so-called elastic container registry the\n",
       "25:09 place in AWS where you can store your\n",
       "25:12 docker images in the repositories. Uh\n",
       "25:16 basically this is the main uh let's say\n",
       "25:18 lending zone for SageMaker estimators.\n",
       "25:22 Doesn't matter what kind of estimators\n",
       "25:23 you use in SageMaker either it's native\n",
       "25:26 estimators uh from SageMaker from AWS\n",
       "25:30 team for example XG boost or lineer\n",
       "25:33 learner or um uh K means etc in in their\n",
       "25:38 realization. Yeah. Or this is the custom\n",
       "25:41 estimators that you would like to\n",
       "25:43 produce yourself.\n",
       "25:45 uh all of this estimators uh will land\n",
       "25:49 on this ECR um landing zone on this in\n",
       "25:52 this repositories. Therefore, you just\n",
       "25:54 need to have the information about the\n",
       "25:57 image URI, which is basically the um the\n",
       "26:01 um let's say the location where you can\n",
       "26:04 find your uh image that you would like\n",
       "26:06 to use and you just uh can use this u\n",
       "26:10 image u for um working with the\n",
       "26:15 SageMaker estimator.\n",
       "26:18 Yeah. And regarding the uh container, so\n",
       "26:22 container application what is basically\n",
       "26:24 stored in our main folder of SageMaker\n",
       "26:26 estimator. So this is the uh two scripts\n",
       "26:30 which I told you before train and\n",
       "26:32 predictor. So train is the main scripts\n",
       "26:34 that you're using for training your ML\n",
       "26:36 models. Um so it also can be combined\n",
       "26:39 with the additional steps such as uh the\n",
       "26:42 procedure of feature selection or\n",
       "26:45 prep-processing. Uh so there are variety\n",
       "26:49 of ways how to do that and the variety\n",
       "26:51 ways of how to um use different\n",
       "26:54 frameworks. Yeah. So for example you can\n",
       "26:56 in our current uh webinar we will\n",
       "26:59 discuss regarding using skyit learn but\n",
       "27:03 uh nothing stops you from using some\n",
       "27:05 other frameworks like tensorflow or kas.\n",
       "27:08 Um in the predictor scripts the model\n",
       "27:11 prediction basically happening. Yeah. So\n",
       "27:14 the model prediction which is basically\n",
       "27:16 uh combined with a flask application. So\n",
       "27:19 flask um wrapper application. Yeah. And\n",
       "27:24 uh um so we can imagine this as a bottom\n",
       "27:28 layer. So the train and predict which is\n",
       "27:30 wrapped in the flask application\n",
       "27:32 combining uh this to the u flask web\n",
       "27:36 services and then afterwards u the uh\n",
       "27:41 top layer and the top layer basically\n",
       "27:43 would be u as we discussed our server\n",
       "27:46 and jinx and our uh load balancer\n",
       "27:49 unicorn.\n",
       "27:51 Um therefore we need a couple more files\n",
       "27:55 to actually start the server and this is\n",
       "27:58 happening in the serve file which we\n",
       "27:59 starting the server and specify the\n",
       "28:01 parameters uh and start the unicorn\n",
       "28:04 which is happening in the vcgi umpi\n",
       "28:09 script. And finally we need the uh\n",
       "28:12 configuration file for the engin which\n",
       "28:14 we can set up the settings for jinx\n",
       "28:17 master. Usually uh it um help us working\n",
       "28:22 with the multiply workers in terms of\n",
       "28:24 multip uh prep processing.\n",
       "28:27 Um so let's dive into the codes go\n",
       "28:32 deeper to that. So uh let's start with\n",
       "28:34 the SageMaker estimator itself or this\n",
       "28:37 folder where we have our code. So uh we\n",
       "28:41 start from the bottom going to the top.\n",
       "28:43 So from the bottom. So this is our train\n",
       "28:47 uh scripts. So basically we here um\n",
       "28:51 making the simplistic way. So the\n",
       "28:53 simplistic way of um writing ML code in\n",
       "28:57 Python. So meaning that we need to uh be\n",
       "29:01 aware of having a lot of entities in uh\n",
       "29:04 one script, a lot of classes or a lot of\n",
       "29:06 functions. So and we trying to make the\n",
       "29:09 model structure uh simplistic model\n",
       "29:12 structure with uh not many entities\n",
       "29:15 inside of each of the structure. So for\n",
       "29:18 example here we for the training just\n",
       "29:20 use one uh function. So for example if\n",
       "29:24 we would like to introduce something\n",
       "29:25 more specific like a preprocessing here\n",
       "29:28 uh so it would be better to have the\n",
       "29:30 another script uh that we can combine\n",
       "29:33 with this one. So we import the\n",
       "29:35 libraries\n",
       "29:37 And here as I already mentioned we will\n",
       "29:39 work with the skyitle learn as our go-to\n",
       "29:41 framework. Um first of all and very\n",
       "29:45 important we need to specify the\n",
       "29:46 prefixes and the path paths for sage\n",
       "29:50 maker. This is uh basically uh we need\n",
       "29:54 to uh to be able to help sage maker to\n",
       "29:57 understand the channels that he should\n",
       "30:01 use uh while running our ML Docker\n",
       "30:04 application. Yeah. So we need to\n",
       "30:06 identify the prefix which will be opt ML\n",
       "30:09 and we need to identify the main three\n",
       "30:12 paths. Yeah. So uh the main three\n",
       "30:15 channels which I described uh above. So\n",
       "30:18 it's like input output and the model\n",
       "30:21 path. Um then we basically select the\n",
       "30:24 channel name training. Um and uh select\n",
       "30:28 the training path. So then is basically\n",
       "30:31 the function train which is uh quite\n",
       "30:34 simple in this case. Um in this example\n",
       "30:39 I use the um\n",
       "30:42 uh standard kegle data set regarding\n",
       "30:45 hard failures. So this is the question\n",
       "30:47 of the the task of binary classification\n",
       "30:50 problem whether it will be hard failure\n",
       "30:52 or not. Um therefore\n",
       "30:55 basically it's quite um um\n",
       "31:00 quite uh toy data sets. So quite a small\n",
       "31:03 one. So but you are free to use your own\n",
       "31:06 data sets. I can applying this code to\n",
       "31:09 your existing problems. Yeah. So we\n",
       "31:12 start basically with the uh checking the\n",
       "31:15 input file directory and uh checking\n",
       "31:18 whether are like files\n",
       "31:21 u existing there or not. Yeah. So uh\n",
       "31:23 basically if there are no files we're\n",
       "31:25 raising the error. If there are some\n",
       "31:26 files we just read them and prepare the\n",
       "31:29 x and y uh vectors for our training.\n",
       "31:34 Yeah. So um afterwards basically uh here\n",
       "31:38 is the example of why we actually need\n",
       "31:42 to use this custom approach. For example\n",
       "31:45 here we decided to use uh so-called\n",
       "31:48 voting classifier. Yeah. which is\n",
       "31:50 basically not introduced in Sage Maker\n",
       "31:53 uh from the scratch. Therefore, we uh\n",
       "31:56 use the custom approach. Yeah. So,\n",
       "31:58 voting classifier um we combine three\n",
       "32:01 models which is basically support vector\n",
       "32:03 machine with the probabilistic\n",
       "32:05 classifier. Yeah. Logistic regression\n",
       "32:07 and random forest classifier with a\n",
       "32:10 voting type equals soft meaning that we\n",
       "32:12 would uh not the predict classes itself.\n",
       "32:15 We will predict the probabilities and\n",
       "32:16 then based on the probabilities of these\n",
       "32:19 three models, three estimators. We will\n",
       "32:23 um understand the\n",
       "32:25 uh final probability uh by uh using mean\n",
       "32:29 of all of this average of all of this\n",
       "32:32 probabilities of all of this estimators.\n",
       "32:35 Then after defining the voting\n",
       "32:37 classifier, we need to um specify for\n",
       "32:40 example the grid search procedure\n",
       "32:42 because I mean um if you start with the\n",
       "32:46 this approach, it would be also nice to\n",
       "32:48 have um the um desirable parameters.\n",
       "32:53 Yeah, that's the the uh fitted\n",
       "32:55 parameters for your model. And basically\n",
       "32:57 we can do that in simplistic uh way\n",
       "33:02 using uh skyit learn grid search. Of\n",
       "33:05 course there are also other ways um\n",
       "33:08 possible but we keep it uh skyit learn\n",
       "33:11 style today. So therefore we actually\n",
       "33:14 after creating the voting classifier\n",
       "33:16 create the params grid uh and specifying\n",
       "33:19 the parameters and the range of the\n",
       "33:22 values that we need to check and we need\n",
       "33:24 to uh compare and then basically uh\n",
       "33:27 using grid search CV putting the\n",
       "33:29 estimators parameters and the desirable\n",
       "33:32 number of the uh cross validation steps.\n",
       "33:35 Then we feed the model and the result of\n",
       "33:37 this fitted model we just uh save um as\n",
       "33:41 a pickle dump\n",
       "33:44 um in the u uh in the file then this\n",
       "33:48 file we will use for the exact\n",
       "33:50 prediction. So this is also one very\n",
       "33:53 important uh point that uh sagemaker\n",
       "33:56 estimator is designed like that you need\n",
       "33:59 to use the main function without having\n",
       "34:02 the main function it won't work. Um so\n",
       "34:04 this is basically the uh the idea and\n",
       "34:07 the inner design of SageMaker that you\n",
       "34:10 need to put uh the main function and\n",
       "34:12 then put the function which you would\n",
       "34:14 like to execute uh during the run. So\n",
       "34:17 here for example we will execute only\n",
       "34:19 this uh function that we have the train\n",
       "34:21 function. This is regarding train\n",
       "34:24 scripts. Now we go to the uh predictor.\n",
       "34:27 Yeah. And the predictor in our case\n",
       "34:29 would be combined with the flask flask\n",
       "34:31 application. So first of all we also\n",
       "34:34 specifying the prefixes for the\n",
       "34:36 SageMaker ML Docker execution opt ML and\n",
       "34:40 uh setting the path to the model because\n",
       "34:43 we would like to um we would like to use\n",
       "34:47 the predefined weights for that. So\n",
       "34:50 basically take the weights and then use\n",
       "34:52 it for the prediction. And um after that\n",
       "34:55 we define the simple class\n",
       "34:59 um with the helper function let's say\n",
       "35:01 get model and predict. So why do we need\n",
       "35:04 that? So basically get model will help\n",
       "35:06 us to understand whether uh the model is\n",
       "35:09 exists whether we already having the uh\n",
       "35:12 fitted and saved uh information about\n",
       "35:14 our weights in a pickle format that we\n",
       "35:17 can take and if it's there we can just\n",
       "35:20 evoke the predict method and making the\n",
       "35:23 prediction against our endpoint. Yeah.\n",
       "35:27 So afterwards we just um specify the uh\n",
       "35:31 flask uh flask application and then\n",
       "35:34 basically uh the things has started.\n",
       "35:36 Yeah. So basically we identifying the\n",
       "35:40 roots the endpoints uh which we\n",
       "35:43 discussed uh before. So uh the endpoint\n",
       "35:46 called pink which u supports get method\n",
       "35:51 uh and uh endpoint invocation which\n",
       "35:53 supports uh post method. Yeah. So\n",
       "35:56 regarding ping so basically we check\n",
       "35:58 whether uh if our get model is not none\n",
       "36:03 then basically we receive status equal\n",
       "36:05 to 200 in case of success. If it's okay\n",
       "36:09 if it's none uh then we basically\n",
       "36:11 receive 404 in case of an error and send\n",
       "36:15 it as a flask response back to the\n",
       "36:17 client. uh then basically the main for\n",
       "36:20 us\n",
       "36:22 uh endpoint this invocation which is\n",
       "36:24 working with the\n",
       "36:26 with the post method. So we first of all\n",
       "36:30 checking our request from the flask\n",
       "36:33 regarding content type. This we do\n",
       "36:36 because the major of the SageMaker estim\n",
       "36:39 estimators are working with the uh text\n",
       "36:42 um / CSV format. Therefore, we need to\n",
       "36:46 check that prior to use it. Yeah. So, if\n",
       "36:49 it's text/ CSV, then we basically take\n",
       "36:52 this data uh using bytes and then\n",
       "36:56 basically read it as a CSV file and\n",
       "36:58 create the data. Otherwise, so if it's\n",
       "37:02 not a CSV format, then we uh send in the\n",
       "37:05 response that our predictor only\n",
       "37:08 supports CSV data. If it's go fine and\n",
       "37:11 we receive the data, so we can actually\n",
       "37:14 use it in the predict method of our\n",
       "37:17 scoring service class\n",
       "37:20 afterwards after creating the\n",
       "37:22 predictions. So we basically can um\n",
       "37:25 prepare them before actually send it\n",
       "37:28 back in the flask response to the\n",
       "37:30 client. So this is basically will be the\n",
       "37:32 response the results the success code\n",
       "37:34 and the type text CSV. So this is\n",
       "37:38 basically two scripts that uh creating\n",
       "37:40 the let's say the core of our program.\n",
       "37:43 So this is the major script that we\n",
       "37:45 always will be um will change you know\n",
       "37:48 during our work. Yeah. So this is the\n",
       "37:50 train procedure and predict with the\n",
       "37:53 flask application above. Yeah. So this\n",
       "37:55 is this combination creates the um web\n",
       "38:00 uh flask application. Yeah. So with the\n",
       "38:02 rest API\n",
       "38:04 structure. So then basically uh going to\n",
       "38:08 the top\n",
       "38:10 um and uh let's quickly look at the\n",
       "38:14 serve file. So the surf file uh I have\n",
       "38:17 to admit that is basically uh kind of\n",
       "38:20 going through the projects. Yeah. From\n",
       "38:22 one project to another project because\n",
       "38:24 uh usually there are not many things\n",
       "38:27 that you would um change on the server\n",
       "38:29 side. So of course you can change for\n",
       "38:31 example the timeout variable. Yeah. to\n",
       "38:35 be able to understand uh when you should\n",
       "38:38 uh kill the process for example kill the\n",
       "38:40 server uh or for example you should also\n",
       "38:44 be able to specify you can specify the\n",
       "38:46 number of workers that's uh um can\n",
       "38:50 execute for example defining the number\n",
       "38:53 of workers based on the multipprocessing\n",
       "38:55 CPU count yeah but uh otherwise it's\n",
       "38:59 pretty standard so the idea that here\n",
       "39:01 we're just starting the server uh which\n",
       "39:04 is starting the engine server and uh\n",
       "39:08 unicorn basically and it's after uh\n",
       "39:11 running it. So your server has started\n",
       "39:13 and uh is ready for receiving requests\n",
       "39:17 and sending back the u responses\n",
       "39:22 and uh basically the configuration of\n",
       "39:25 engin uh usually this is also say\n",
       "39:29 predefined and there's not many things\n",
       "39:31 that you would um change uh from project\n",
       "39:35 to project. Yeah. So of course you can\n",
       "39:37 change some things like uh worker\n",
       "39:39 processes. Yeah. But the main things or\n",
       "39:42 for example timeouts. Yeah. But the main\n",
       "39:44 things for example where the logs stored\n",
       "39:47 where the access logs or error logs are\n",
       "39:49 stored they mostly state the same from\n",
       "39:52 one project to another one. Um so this\n",
       "39:56 is basically the major um structure of\n",
       "40:00 the um of our folder with our code\n",
       "40:03 SageMaker estimator. Then basically we\n",
       "40:06 go back to the root directory and here\n",
       "40:09 we can see the uh basically the two\n",
       "40:12 files that would help us actually to\n",
       "40:15 build and deploy build and push sorry\n",
       "40:18 the um docker image to AWS. So let's\n",
       "40:23 start with the docker file. So the\n",
       "40:25 docker files look like that. So\n",
       "40:27 basically uh we using here operating\n",
       "40:31 system Ubuntu with certain version. Then\n",
       "40:34 basically we using run command for\n",
       "40:37 updating\n",
       "40:39 uh the packages on it and uh install the\n",
       "40:42 packages that we would uh like to use in\n",
       "40:45 this current uh orchestration. So for us\n",
       "40:48 um will be important to have our top\n",
       "40:51 four numpy pi panda skitler and uh so\n",
       "40:55 this is for the bottom layer and for the\n",
       "40:58 top layer we need to have n jinx uh\n",
       "41:00 flask and unicorn with gent afterwards\n",
       "41:04 we identifying the environment variables\n",
       "41:08 copy the main folder to the opt program\n",
       "41:11 and set it as a work directory. Then\n",
       "41:14 basically after this uh step all of the\n",
       "41:19 further on operation will be uh\n",
       "41:21 proceeding in this directory opt program\n",
       "41:24 uh as a work directory.\n",
       "41:27 Uh so this basically file will be used\n",
       "41:29 by docker build when we when we started\n",
       "41:33 and then basically the shell script\n",
       "41:35 docker to ECR. So how it's look like? So\n",
       "41:39 um as I already told it consist of two\n",
       "41:41 parts two logical parts. So the first\n",
       "41:43 part we working with the uh information\n",
       "41:47 of our um AWS account. So first of all\n",
       "41:51 we need to specify the name for our\n",
       "41:53 image which we would like to use and\n",
       "41:55 also afterwards we need to specify to\n",
       "41:58 get the information about the account u\n",
       "42:01 that we will use and the region. Yeah.\n",
       "42:04 So basically we here using AWS configure\n",
       "42:08 comment to get region for example.\n",
       "42:10 Therefore, it should be um identified\n",
       "42:12 beforehand to be able to use it um use\n",
       "42:16 it as it is because you need to specify\n",
       "42:17 your for example public and private key\n",
       "42:20 uh to be able to run this command. Um\n",
       "42:24 afterwards\n",
       "42:25 you create AWS ECR describe repository\n",
       "42:29 commands and if there is uh existing\n",
       "42:32 repository you just put it there. If\n",
       "42:34 it's not exist, you just create the\n",
       "42:36 repository in ECR\n",
       "42:39 u and take all this information. So this\n",
       "42:41 is basically the end of the part one.\n",
       "42:43 Then it's part two which is basically\n",
       "42:46 docker part. Then you go to the docker\n",
       "42:49 build. So you execute docker build\n",
       "42:51 command uh creating the image. Then you\n",
       "42:54 basically tag this image with the full\n",
       "42:57 name. So which is the information about\n",
       "42:59 your account, about your region, about\n",
       "43:02 the image name and about the uh tag\n",
       "43:05 latest which will help SageMaker\n",
       "43:07 understand uh what exact uh image uh it\n",
       "43:12 should use you know for production um\n",
       "43:15 the outcome and then the final step is\n",
       "43:18 docker push full name. So basically take\n",
       "43:21 the uh docker image and push it to the\n",
       "43:25 uh to this uh location to the full name\n",
       "43:27 account region uh image tag. Yeah. So\n",
       "43:32 and this is basically how it goes after\n",
       "43:36 we um run shell docker tor.shell\n",
       "43:41 and the name of our image. So for\n",
       "43:45 example here I use name called modeling.\n",
       "43:47 So we received this data uh this docker\n",
       "43:50 image in our ECR repository on AWS and\n",
       "43:54 afterwards basically we can start\n",
       "43:56 working with that. So let's go to the um\n",
       "43:59 AWS itself and uh I will show you how\n",
       "44:01 you can work with that. So um here is\n",
       "44:06 basically AWS HMaker. I think everybody\n",
       "44:09 or most of uh people has the experience\n",
       "44:14 with that seen that. So here is\n",
       "44:15 basically the uh notebook instances\n",
       "44:18 which you can use for which is basically\n",
       "44:20 the main uh thing for SageMaker. So and\n",
       "44:23 you can think of it as a let's say EC2\n",
       "44:26 instances like a virtual machine which\n",
       "44:28 has the predefined um installation such\n",
       "44:32 as Python such as Anaconda different\n",
       "44:35 versions yeah for example different\n",
       "44:37 other uh specific frameworks such as\n",
       "44:40 TensorFlow PyTorch\n",
       "44:43 um etc. Yeah. Uh so and basically um you\n",
       "44:47 can use uper notebooks there for running\n",
       "44:52 uh for doing research and doing the\n",
       "44:54 deployment. So usually this is the\n",
       "44:56 common procedure when you work on the\n",
       "45:00 Amazon SageMaker that you creating the\n",
       "45:02 notebook notebook on the instance and uh\n",
       "45:06 run your comments there. So let's go to\n",
       "45:08 the uh notebook and uh we'll see how it\n",
       "45:13 how we can work with that. So this is\n",
       "45:15 basically our notebook SageMaker\n",
       "45:18 estimator.\n",
       "45:20 So how it's uh how we should work with\n",
       "45:22 that. So basically um we should use the\n",
       "45:26 following libraries for us will be\n",
       "45:28 important also to use SageMaker\n",
       "45:30 libraries. Um this is the two important\n",
       "45:33 steps that we need to um identify our\n",
       "45:36 role. So get execution role um because\n",
       "45:39 the different roles in AWS have the\n",
       "45:41 different rights and accesses uh and\n",
       "45:44 then identify the SageMaker session.\n",
       "45:47 This will help us to uh run certain\n",
       "45:50 estimators. Yeah. So which we will do\n",
       "45:53 later on. So um beforehand we can\n",
       "45:56 actually connect to the S3 uh to the\n",
       "45:59 bucket which I prepared for this\n",
       "46:01 presentation and look at the data which\n",
       "46:03 we will using in this um in this uh code\n",
       "46:08 session. Yeah. So we read it and then\n",
       "46:11 basically can look at it with the head\n",
       "46:14 command. Yeah. So we have here is the u\n",
       "46:18 target variable binary uh hardware hard\n",
       "46:21 failure and also the other features that\n",
       "46:25 we will use in um in terms of our uh\n",
       "46:29 training. Yeah. So here is the info\n",
       "46:33 commands. So it's quite small data set\n",
       "46:35 for presentational\n",
       "46:38 um purposes and then basically we go to\n",
       "46:41 the uh to the most interesting part.\n",
       "46:44 Yeah. So right now we uh using both 3\n",
       "46:47 which is basically the library that's\n",
       "46:49 enabling uh programming API between\n",
       "46:52 Python 3 from one side and AWS from\n",
       "46:55 another side. We uh using STS uh having\n",
       "46:59 the information about the account which\n",
       "47:02 we are using and about the region uh in\n",
       "47:05 which the model will be hosted. Then uh\n",
       "47:09 we specify uh image URI. This is\n",
       "47:12 basically the location where uh we can\n",
       "47:16 find our docker image yeah with our\n",
       "47:20 voting classifier that we prepared for\n",
       "47:22 this session. So it called uh modeling\n",
       "47:26 latest and uh take the all u information\n",
       "47:30 regarding account and region from there.\n",
       "47:34 After we specify it we are ready to\n",
       "47:36 actually\n",
       "47:38 uh make our skyitle learn estimator uh\n",
       "47:42 sagemaker estimator transform it to\n",
       "47:44 sagemaker estimator and for that we\n",
       "47:46 actually use this command. So we specify\n",
       "47:49 estimator.estimator estimator and put\n",
       "47:51 the um certain parameters inside. So we\n",
       "47:54 put image uri as an input parameter role\n",
       "47:57 which we use for the execution the\n",
       "47:59 number of instances for the training and\n",
       "48:02 the um and the instance alias which um\n",
       "48:08 which telling us the amount of uh cores\n",
       "48:11 and uh memory that is using on this\n",
       "48:14 instance. The output path is basically\n",
       "48:16 the three path location where SageMaker\n",
       "48:19 will store all the artifacts after the\n",
       "48:23 training and SageMaker session which is\n",
       "48:26 which we'll be using for uh for enabling\n",
       "48:29 it. Afterwards we just run fit command\n",
       "48:32 on the data location where our train and\n",
       "48:35 test data store uh stores. So and then\n",
       "48:39 basically you have the variables so the\n",
       "48:41 information the output of how it goes.\n",
       "48:43 So it's starting the training job\n",
       "48:45 launching the services prepared and\n",
       "48:48 basically do the training. So after it's\n",
       "48:50 finished you receive the information the\n",
       "48:52 training completes uh and uh information\n",
       "48:55 regarding how much time uh does it take\n",
       "48:58 for training uh the uh SageMaker\n",
       "49:00 estimator. So now we are basically uh\n",
       "49:03 ready. So we transformed from skankit\n",
       "49:06 learn custom estimator which was in the\n",
       "49:09 docker container.\n",
       "49:11 um we transformed it to the SageMaker\n",
       "49:13 estimator. Now all that we have in\n",
       "49:15 SkyitLarn\n",
       "49:17 estimator. Uh we have right now also on\n",
       "49:21 SageMaker estimator. And basically we\n",
       "49:24 can deploy it very easily with one line\n",
       "49:26 of code using the deploy method and\n",
       "49:29 specifying once again the number of\n",
       "49:32 instances, the uh type of instances and\n",
       "49:36 the type of serializer. Yeah, due to the\n",
       "49:39 fact that we are working with CSV data.\n",
       "49:41 So we select CSV serializer here. So\n",
       "49:44 when we um receive the output that it's\n",
       "49:49 ready. So when then actually we can\n",
       "49:51 start using it. So and for that\n",
       "49:54 basically we can uh do it right here in\n",
       "49:56 the notebook and u check whether it's\n",
       "49:58 working or not. So uh we take the\n",
       "50:01 predictor and use the method predict\n",
       "50:04 against uh the certain values. So\n",
       "50:06 afterwards it uh provides us the output\n",
       "50:11 which will be in the bytes format.\n",
       "50:12 Therefore we have to make some\n",
       "50:15 preparations via string to CSV and then\n",
       "50:19 uh receive the uh receive the output of\n",
       "50:22 our of our classification here. Yeah.\n",
       "50:27 So here is the output of our binary\n",
       "50:29 classification.\n",
       "50:31 And basically of course we can procedure\n",
       "50:34 with the following research. We can\n",
       "50:36 calculate the metrics for example for\n",
       "50:38 classification can calculate accuracy\n",
       "50:41 score uh precision recall F1 score for\n",
       "50:45 regression we can calculate RMSSE or\n",
       "50:48 Maya score um so um this is very\n",
       "50:53 interesting but for us would be also\n",
       "50:55 very important question what to do next\n",
       "50:57 how we can basically deploy it in terms\n",
       "51:00 of using it yeah how we can basically\n",
       "51:02 make this model um used with our clients\n",
       "51:07 or without or with our uh departments.\n",
       "51:11 Yeah. And for that basically it's not\n",
       "51:14 many works to do. So this is actually\n",
       "51:16 two steps that we need to make. We need\n",
       "51:19 to specify the so-called lambda function\n",
       "51:21 and we need to specify AP gateway for uh\n",
       "51:25 being let's say front end for this uh\n",
       "51:28 AWS model.\n",
       "51:30 Um and then basically this is the\n",
       "51:33 example of payload which I will be using\n",
       "51:36 for um sending to our um endpoint and uh\n",
       "51:42 hopefully receiving the output result.\n",
       "51:45 So let's first go to the lambda\n",
       "51:47 function. It's called sagemaker invoke.\n",
       "51:50 Um and basically here is the um is the\n",
       "51:56 information. So\n",
       "52:00 um here we can see the lambda function\n",
       "52:02 itself. So the lambda handler\n",
       "52:06 um but uh for us it's um important to\n",
       "52:10 start from this part. Yeah. So we're\n",
       "52:12 starting with the identifying the\n",
       "52:14 endpoint name. The endpoint name uh will\n",
       "52:17 be added as an environmental variable.\n",
       "52:20 uh that would basically help our Lambda\n",
       "52:24 um connect with the SageMaker and make\n",
       "52:26 some operation and for that we need to\n",
       "52:29 establish runtime using both 3 uh for\n",
       "52:32 connection uh Python to AWS. So for the\n",
       "52:37 B3 for the client we use runtime\n",
       "52:38 SageMaker and afterwards we can invoke\n",
       "52:40 the things.\n",
       "52:42 So a couple of words regarding Lambda\n",
       "52:45 function. Lambda function is a very very\n",
       "52:47 lightweight and uh event based function\n",
       "52:49 on AWS which is triggered by some events\n",
       "52:52 and you can write in here in the\n",
       "52:54 different languages such as for example\n",
       "52:56 node uh gs or python so here we use the\n",
       "52:59 python version 3.8 eight\n",
       "53:03 um and of course for example there is\n",
       "53:05 super lightweight so you don't have an\n",
       "53:07 access um from the scratch for example\n",
       "53:10 to pandas or to numpy libraries which\n",
       "53:13 are essential for example for some uh\n",
       "53:15 scientific packages in python such as\n",
       "53:17 anaconda so and you can see it here so\n",
       "53:20 we use pretty standard pythonic models\n",
       "53:23 here so how is the uh lambda is\n",
       "53:27 structured yeah so we get the data from\n",
       "53:29 the um from our request or JSON request\n",
       "53:33 and put it to the uh payload. So we take\n",
       "53:37 the data from uh so it's like coming in\n",
       "53:40 the key value u format. So we take the\n",
       "53:43 data from uh key data take the values\n",
       "53:46 which will be our x feature vector and\n",
       "53:50 uh that would be our payload that we\n",
       "53:52 will send later on to the model. So for\n",
       "53:54 doing that we need to uh make the\n",
       "53:56 runtime SageMaker invoke endpoint to\n",
       "53:59 actually you know um uh start uh invoke\n",
       "54:03 the endpoint and specifying the name of\n",
       "54:06 this endpoint which we're specifying in\n",
       "54:08 the environment variables here uh\n",
       "54:11 content type text CSV and the body which\n",
       "54:14 will be our payload the information\n",
       "54:15 about the features that we will be\n",
       "54:17 using. So and also some information\n",
       "54:20 whether it will be zero which is not\n",
       "54:22 failure and if it's like a one then it\n",
       "54:24 will be a failure.\n",
       "54:26 Um and the final step is actually uh\n",
       "54:31 implementing the AP gateway. So creating\n",
       "54:34 the AP gateway in our sense we will be\n",
       "54:36 using REST API service because it's a\n",
       "54:38 REST API um web model. Um and we need to\n",
       "54:43 identify the post request the post\n",
       "54:45 request because we need to use the\n",
       "54:47 invocation endpoint and receive the\n",
       "54:49 information about that about our\n",
       "54:52 predicted variable. Uh this post methods\n",
       "54:55 we need to align we need to combine with\n",
       "54:57 the uh with the lambda function that we\n",
       "55:00 uh talked before.\n",
       "55:02 Um and basically that's it. So the for\n",
       "55:05 the u major of operations. Afterwards we\n",
       "55:10 just can use uh request body for the\n",
       "55:13 testing\n",
       "55:14 uh and put for example our\n",
       "55:18 uh our data here. So for the request\n",
       "55:21 body and receive the test.\n",
       "55:26 Yeah. So here it's working. So we\n",
       "55:27 receive the uh status called 200 which\n",
       "55:30 is success. So which is latency and\n",
       "55:33 response body. So failure in our case\n",
       "55:36 it's uh basically class number one. So\n",
       "55:40 so we have the not failure and failure\n",
       "55:43 for example.\n",
       "55:44 So and um the AP gateway also provide\n",
       "55:48 the information from the Python code\n",
       "55:50 regarding errors. You can make the error\n",
       "55:52 handling to make it more uh smoother.\n",
       "55:55 For example, we can also like create\n",
       "55:57 some let's say error here. Um\n",
       "56:02 yeah and it creates uh and it creates\n",
       "56:04 the error message that you can uh later\n",
       "56:06 on uh later on handle. Yeah.\n",
       "56:13 Yeah. So and uh basically to summarize\n",
       "56:16 this part um we have talked about so how\n",
       "56:22 the uh custom algorithm on the certain\n",
       "56:26 framework such as skyit learn in our\n",
       "56:28 example should be look like how we\n",
       "56:31 should uh train the model and how we\n",
       "56:34 should combine it with the top level\n",
       "56:37 features as web server and uh vcgi and\n",
       "56:41 how we should uh packets into the docker\n",
       "56:45 image and push it to AWS. Yeah. Then we\n",
       "56:49 basically um discuss regarding SageMaker\n",
       "56:52 how the exact um how exact\n",
       "56:58 um\n",
       "57:00 framework is transforming to the um\n",
       "57:05 SageMaker estimator. how we deploy the\n",
       "57:07 model and how we actually make it live\n",
       "57:10 using lambda and AP gateway.\n",
       "57:13 So and for example in the end so how it\n",
       "57:16 will work as a final step um so client\n",
       "57:20 can send the post request with the x\n",
       "57:21 feature vector on the uh public uh uh\n",
       "57:27 DNS. Yeah. Uh and uh um receiving the\n",
       "57:33 output. So the uh using AP uh gateway\n",
       "57:37 and using lambda for um invocation of\n",
       "57:40 sage maker estimator.\n",
       "57:43 So after the session I will send you the\n",
       "57:46 um link to the repository that you can\n",
       "57:48 use for reading and for uh practical\n",
       "57:52 work for your um examples for your use\n",
       "57:56 cases and also we have a plan uh if it\n",
       "57:59 will be interesting for you. So please\n",
       "58:01 also share your opinion uh in the data\n",
       "58:04 talks club uh regarding u the next um\n",
       "58:09 the next uh webinars regarding SageMaker\n",
       "58:12 because we had the idea to have at least\n",
       "58:14 two webinars uh for the SageMaker. So\n",
       "58:17 the second webinar should cover the\n",
       "58:19 topics of proper data prep-processing\n",
       "58:22 uh using pipelines and using\n",
       "58:24 transformers and how to combine these\n",
       "58:26 things with the SageMaker. And the third\n",
       "58:29 um event should cover the topics uh\n",
       "58:33 regarding uh automation of the um of the\n",
       "58:38 sage maker. So uh we will discuss we\n",
       "58:42 plan to discuss regarding step functions\n",
       "58:44 there regarding uh combination airflow\n",
       "58:48 plus sage maker there. So feel free to u\n",
       "58:52 put your opinion.\n",
       "58:54 Um and I think that is u more or less it\n",
       "58:58 from my side. So if you have questions\n",
       "58:59 so let's discuss.\n",
       "59:02 Yeah thanks Mitri. Uh yes we do have a\n",
       "59:04 couple of questions. So um have them on\n",
       "59:07 slider. Um so I think we can pick top\n",
       "59:10 three and then u maybe you can answer\n",
       "59:13 the rest in in Slack.\n",
       "59:15 Yeah.\n",
       "59:15 So first the most popular question is\n",
       "59:18 how models are updated. Sage Maker\n",
       "59:20 doesn't allow redeploying models with\n",
       "59:22 the same endpoint name.\n",
       "59:23 Um Do you have a separate service\n",
       "59:26 tracking endpoints or how do you do\n",
       "59:27 this?\n",
       "59:28 Uh basically this are couple of things\n",
       "59:30 how uh there's a couple of ways how you\n",
       "59:32 can do that. So before the um um\n",
       "59:35 existing of step functions uh basically\n",
       "59:38 the only way how you can do that it was\n",
       "59:40 the using uh extensive using of lambda\n",
       "59:44 functions. So you use lambda function\n",
       "59:45 for um let's say retraining and adding\n",
       "59:49 this in the end of retraining job to\n",
       "59:51 update the uh certain certain endpoint\n",
       "59:55 that was like a uh was a big problem big\n",
       "59:58 issue. Therefore uh recently so around\n",
       "1:00:02 one year um ago so the step function for\n",
       "1:00:06 Sage Maker were announced and basically\n",
       "1:00:09 this is uh also from AWS considered to\n",
       "1:00:13 be as a main approach. So you use step\n",
       "1:00:16 functions and uh you use them for\n",
       "1:00:19 basically creating the retraining uh of\n",
       "1:00:23 your model that will be also updating\n",
       "1:00:25 your um endpoint because I also know\n",
       "1:00:28 that this is the very important\n",
       "1:00:29 questions uh because there are lots of\n",
       "1:00:32 question on the forums on the uh stock\n",
       "1:00:34 overflow regarding that and not many\n",
       "1:00:36 answers about that. Yeah. So actually\n",
       "1:00:38 the principle right now that is um that\n",
       "1:00:41 you can use step functions for doing\n",
       "1:00:43 that and for\n",
       "1:00:46 uh to make the process automate to\n",
       "1:00:49 automate the process you can uh use\n",
       "1:00:52 either lambda functions or uh basically\n",
       "1:00:55 cloudatch to make uh kind of\n",
       "1:00:58 scheduleuler scheduleuler to make it um\n",
       "1:01:01 automate. Yeah. So this is basically um\n",
       "1:01:06 the main process and the second way how\n",
       "1:01:10 you can do that it's using airflow for\n",
       "1:01:13 example and this is basically the our\n",
       "1:01:15 idea of the next talk so if it's this\n",
       "1:01:18 point I guess uh will be interesting for\n",
       "1:01:21 many of you so we can basically uh cover\n",
       "1:01:23 it in the next session and discuss\n",
       "1:01:26 regarding step function as a main\n",
       "1:01:28 approach and uh cover a bit uh airflow\n",
       "1:01:31 topic\n",
       "1:01:33 Okay.\n",
       "1:01:34 Can I further clarify on this? Um, so\n",
       "1:01:37 when you're changing when you update the\n",
       "1:01:39 model and endpoint, do you control the\n",
       "1:01:42 endpoint uh and the swapping of the\n",
       "1:01:43 endpoint on your SageMaker end or does\n",
       "1:01:46 your app have to know which endpoint to\n",
       "1:01:50 call every time you update a model? I'm\n",
       "1:01:52 just wondering where you put the\n",
       "1:01:53 endpoint handling logic. Does SageMaker\n",
       "1:01:55 allow you for that or you have to build\n",
       "1:01:57 it outside of it? So basically right now\n",
       "1:01:59 so once again so basically beforehand it\n",
       "1:02:03 was really a big issue. Yeah it's just\n",
       "1:02:06 like you need to use a lot of lambda\n",
       "1:02:08 function together to um to basically\n",
       "1:02:11 handle the training and uh endpoints\n",
       "1:02:14 right now it's basically constructed all\n",
       "1:02:16 all constructed in the step function. So\n",
       "1:02:18 the step function if you can check that\n",
       "1:02:20 it's basically having the very also nice\n",
       "1:02:23 visual graph uh regarding the process\n",
       "1:02:25 how it starts from the creation of the\n",
       "1:02:28 training job. Yeah. So like updating the\n",
       "1:02:30 create uh the training job creating the\n",
       "1:02:33 model creating the endpoint\n",
       "1:02:34 configuration and creating the endpoint\n",
       "1:02:37 itself. And of course if you would like\n",
       "1:02:39 to use your uh existing model so you\n",
       "1:02:42 don't need to create the endpoint itself\n",
       "1:02:45 once again and once again. So you just\n",
       "1:02:48 specify it in the step function u\n",
       "1:02:51 graphical interface u that you would\n",
       "1:02:53 like to use it uh with a certain\n",
       "1:02:56 endpoint that that you need to only\n",
       "1:02:58 retrain the model do another training\n",
       "1:03:01 job and put this training job in the um\n",
       "1:03:05 in the existing endpoint and uh it also\n",
       "1:03:08 provides even uh more complex um\n",
       "1:03:12 opportunities. Therefore you can also\n",
       "1:03:15 trigger and u uh make evaluation based\n",
       "1:03:18 on the parameters of the model. Yeah. So\n",
       "1:03:20 you can check how the parameters are\n",
       "1:03:23 doing how the model are doing and\n",
       "1:03:24 basically define whether if the\n",
       "1:03:27 parameters for example your F1 score is\n",
       "1:03:29 lower than the previous one you're not\n",
       "1:03:32 updating the endpoint and updating only\n",
       "1:03:34 if it's will be above.\n",
       "1:03:39 Okay. Thank you.\n",
       "1:03:41 Mhm.\n",
       "1:03:43 Um thank you. So um we have another\n",
       "1:03:46 question is uh whether you tried sage\n",
       "1:03:49 maker inference toolkit toolkit or not.\n",
       "1:03:53 Uh and if yes u how is it different from\n",
       "1:03:57 what you did? Yeah, I tried. But\n",
       "1:04:00 basically this is in my opinion I mean\n",
       "1:04:02 of course this is also the good idea to\n",
       "1:04:04 use it but in my opinion um to use it in\n",
       "1:04:08 the way how I described giving you more\n",
       "1:04:11 opportunities and more flexibility.\n",
       "1:04:13 Yeah. So um in terms that you having the\n",
       "1:04:17 access to every component of the ML\n",
       "1:04:21 model starting from the bottom from the\n",
       "1:04:22 core from the training code and u\n",
       "1:04:26 finishing with the configuration of the\n",
       "1:04:28 web server. So you can basically uh get\n",
       "1:04:31 the hand on pulse you know for the every\n",
       "1:04:34 point of the things which is really\n",
       "1:04:37 really cool. So and for for example for\n",
       "1:04:39 us for ODBN it's very important to um to\n",
       "1:04:42 be able to be flexible in terms of\n",
       "1:04:44 tasks. Yeah. Therefore for us is one of\n",
       "1:04:47 the um preferable variants how to how to\n",
       "1:04:51 do.\n",
       "1:04:52 So basically more control and more\n",
       "1:04:54 flexibility.\n",
       "1:04:54 Yeah. Yeah.\n",
       "1:04:56 Okay. Yes. And also I would say that\n",
       "1:04:58 basically if we um go in deeper to that\n",
       "1:05:02 so for example this shell script we can\n",
       "1:05:04 easily um for example we can create\n",
       "1:05:08 CI/CD pipeline for this repository let's\n",
       "1:05:12 say we can use genkins or other tool\n",
       "1:05:14 like circle CI and uh for example for\n",
       "1:05:17 every uh operations for every action\n",
       "1:05:21 inside of the master. Yeah. So for\n",
       "1:05:23 example for commits or um merging the\n",
       "1:05:26 pull requests. So it will send the\n",
       "1:05:29 information via web hook yeah to the um\n",
       "1:05:32 to the uh CI/CD platform and then\n",
       "1:05:36 basically uh it would run uh your shell\n",
       "1:05:39 script and automatically rebuild and\n",
       "1:05:40 push the um the whole um the whole\n",
       "1:05:44 system which is super cool which is\n",
       "1:05:46 super um easy and um uh really nice to\n",
       "1:05:50 work with.\n",
       "1:05:53 Thanks.\n",
       "1:05:55 And um yeah, so we have uh one uh more\n",
       "1:05:59 question that we will answer now and the\n",
       "1:06:01 rest we'll take uh in Slack. So a\n",
       "1:06:04 question from Eugen is uh did you\n",
       "1:06:06 observe a latency difference between\n",
       "1:06:08 calling the predictor object directly\n",
       "1:06:10 like you did in the notebook and calling\n",
       "1:06:13 it through API gateway through the rest\n",
       "1:06:15 API? Did you notice any difference\n",
       "1:06:17 between these two?\n",
       "1:06:18 No, of course there is some difference.\n",
       "1:06:20 So of course there is uh really um it's\n",
       "1:06:23 really depending on the structure of the\n",
       "1:06:26 lambda that we have. Yeah. So for\n",
       "1:06:29 example um we use lambda um in a couple\n",
       "1:06:32 of ways. So we use it in as a like a\n",
       "1:06:35 lightweight evenbased function and we\n",
       "1:06:38 also use a bit u\n",
       "1:06:42 heavyweight lambdas meaning that we\n",
       "1:06:44 apply layer to this lambda for getting\n",
       "1:06:48 access to the packages and do some uh\n",
       "1:06:51 kind of preprocessing inside of the\n",
       "1:06:53 lambda. For example, uh using like would\n",
       "1:06:56 say label encoder um not to train them\n",
       "1:07:00 there but just to load uh pre-trained\n",
       "1:07:04 label encoder load the weights of this\n",
       "1:07:06 label encoder and uh transform the data\n",
       "1:07:10 inside of the of the lambda. Of course,\n",
       "1:07:13 basically if you have this uh on the\n",
       "1:07:16 standard instances, you would with more\n",
       "1:07:19 and more preprocessing steps that you\n",
       "1:07:21 can do in the lambda, you will see a lot\n",
       "1:07:24 of problems and a lot of um you can see\n",
       "1:07:27 the gaps in the latency. So you need to\n",
       "1:07:29 have the balance either with the\n",
       "1:07:31 preprocessing or with the uh or with the\n",
       "1:07:35 machine that you are using for uh for\n",
       "1:07:38 that. U so you put this prep-processing\n",
       "1:07:42 into lambda because it's cheaper in\n",
       "1:07:45 lambda than in SageMaker or\n",
       "1:07:47 yeah uh it's cheaper and usually\n",
       "1:07:50 basically if it's um uh I'm talking\n",
       "1:07:52 about the kind of like a simple things\n",
       "1:07:54 about the simple preprocessing when it's\n",
       "1:07:56 um for example you receiving in the um\n",
       "1:08:00 in the uh requests it's like a string\n",
       "1:08:03 value yeah therefore you can easily do\n",
       "1:08:06 that inside of the lambda immediately\n",
       "1:08:08 because You also have the pre uh fitted\n",
       "1:08:12 prayer trains uh encoder that you can\n",
       "1:08:14 just load and uh uh use it. You even can\n",
       "1:08:19 create couple of lambdas that would you\n",
       "1:08:21 know uh create a chain. For example, the\n",
       "1:08:24 first um uh the first request are coming\n",
       "1:08:28 to the first lambda creating the uh\n",
       "1:08:31 transformation.\n",
       "1:08:33 For example, like some simplistic\n",
       "1:08:34 prep-processing stuff with the strings\n",
       "1:08:36 and then the prep-processed results\n",
       "1:08:38 going to the um core lambda that would\n",
       "1:08:41 create the invoke endpoint procedure.\n",
       "1:08:46 Okay, thank you. So uh with that we will\n",
       "1:08:50 write uh we will wrap up. So thank you\n",
       "1:08:52 very much for presenting in this. Thank\n",
       "1:08:53 you very much for attending and for your\n",
       "1:08:56 questions. We will make sure to answer\n",
       "1:08:58 all the remaining questions in Slack and\n",
       "1:09:02 uh please come to our next events. Thank\n",
       "1:09:05 you.\n",
       "1:09:06 Mhm.\n",
       "1:09:06 So thank you very much. Yes. So see you\n",
       "1:09:08 soon. Chowo chiao.\n",
       "1:09:10 Goodbye.</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The video is a webinar discussing the deployment of machine learning (ML) models using AWS SageMaker, specifically focusing on dockerized ML models.</p>\n",
       "<h3>Key Points:</h3>\n",
       "<ol>\n",
       "<li><p><strong>Introduction and Upcoming Events</strong>:</p>\n",
       "<ul>\n",
       "<li>The event is part of a series of technical discussions scheduled every Tuesday.</li>\n",
       "<li>Topics include customer segmentation, fraud detection with neural networks, and communication skills for data professionals.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Deployment with AWS SageMaker</strong>:</p>\n",
       "<ul>\n",
       "<li>The speaker, Mitri Muzaleki, works as a lead data scientist and discusses the importance of AWS in deploying ML models.</li>\n",
       "<li>The session covers how to use Docker to package ML code and how to deploy it on SageMaker.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Technical Overview</strong>:</p>\n",
       "<ul>\n",
       "<li><strong>Components</strong>:<ul>\n",
       "<li>Models are dockerized and pushed to AWS where SageMaker is used for deployment.</li>\n",
       "<li>The structure involves input, output, and model directories for managing data and configurations.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><strong>Execution Flow</strong>:<ul>\n",
       "<li>Discusses the internal flow of how SageMaker executes docker containers, including its interaction with REST APIs.</li>\n",
       "<li>Highlights the use of Flask for web service management.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Hands-on Demonstration</strong>:</p>\n",
       "<ul>\n",
       "<li>A demonstration is provided, showing how to build, tag, and push Docker images to AWS ECR (Elastic Container Registry).</li>\n",
       "<li>The speaker demonstrates training models using Sklearn and deploying them on SageMaker.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>API Gateway and Lambda Functions</strong>:</p>\n",
       "<ul>\n",
       "<li>Explains how to set up API Gateway to interact with SageMaker models through Lambda functions for real-time predictions.</li>\n",
       "<li>Discusses the latency differences between direct calls to the predictor in SageMaker and using an API Gateway setup.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Q&amp;A Session</strong>:</p>\n",
       "<ul>\n",
       "<li>The session includes questions regarding model update processes, the use of example algorithms, and differences in latency between methods.</li>\n",
       "<li>The final thoughts focus on the flexibility of using custom models and workflows tailored to business requirements.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Conclusion:</h3>\n",
       "<p>The webinar emphasizes the integration of Docker with AWS for the deployment and management of ML models and highlights further learning opportunities through planned events and discussions.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: thanks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>You're welcome! If you have any more questions or need further assistance, feel free to ask. Happy watching!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5731afb-c7a9-462d-8c1e-e80652081a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
