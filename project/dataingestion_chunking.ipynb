{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9c1dd4-04c2-45fa-a6e2-1ef0cfa9b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "client = arxiv.Client()\n",
    "\n",
    "def fetch_arxiv_papers(query, max_results=10):  \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    results = []\n",
    "    for result in search.results():\n",
    "        paper_info = {\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"summary\": result.summary,\n",
    "            \"published\": result.published,\n",
    "            \"url\": result.pdf_url,\n",
    "            \"categories\": result.categories\n",
    "        }\n",
    "        results.append(paper_info)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d548a5-a7df-439a-880d-ea643e3d874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13872/3541550061.py:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    }
   ],
   "source": [
    "papers = fetch_arxiv_papers(query = 'attention', max_results= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d62cc6d-dad4-48c0-8742-212b9a1f0f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?', 'authors': ['Yihao Li', 'Saeed Salehi', 'Lyle Ungar', 'Konrad P. Kording'], 'summary': 'Object binding, the brain\\'s ability to bind the many features that\\ncollectively represent an object into a coherent whole, is central to human\\ncognition. It groups low-level perceptual features into high-level object\\nrepresentations, stores those objects efficiently and compositionally in\\nmemory, and supports human reasoning about individual object instances. While\\nprior work often imposes object-centric attention (e.g., Slot Attention)\\nexplicitly to probe these benefits, it remains unclear whether this ability\\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\\ncould: recognizing which patches belong to the same object should be useful for\\ndownstream prediction and thus guide attention. Motivated by the quadratic\\nnature of self-attention, we hypothesize that ViTs represent whether two\\npatches belong to the same object, a property we term IsSameObject. We decode\\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\\nin ImageNet-supervised models, suggesting that binding is not a trivial\\narchitectural artifact, but an ability acquired through specific pretraining\\nobjectives. We further discover that IsSameObject is encoded in a\\nlow-dimensional subspace on top of object features, and that this signal\\nactively guides attention. Ablating IsSameObject from model activations\\ndegrades downstream performance and works against the learning objective,\\nimplying that emergent object binding naturally serves the pretraining\\nobjective. Our findings challenge the view that ViTs lack object binding and\\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\\nnaturally in a connectionist system.', 'published': datetime.datetime(2025, 10, 28, 17, 57, 5, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24709v1', 'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'q-bio.NC']}, {'title': 'Group Relative Attention Guidance for Image Editing', 'authors': ['Xuanpu Zhang', 'Xuesong Niu', 'Ruidong Chen', 'Dan Song', 'Jianhao Zeng', 'Penghui Du', 'Haoxiang Cao', 'Kai Wu', 'An-an Liu'], 'summary': \"Recently, image editing based on Diffusion-in-Transformer models has\\nundergone rapid development. However, existing editing methods often lack\\neffective control over the degree of editing, limiting their ability to achieve\\nmore customized results. To address this limitation, we investigate the\\nMM-Attention mechanism within the DiT model and observe that the Query and Key\\ntokens share a bias vector that is only layer-dependent. We interpret this bias\\nas representing the model's inherent editing behavior, while the delta between\\neach token and its corresponding bias encodes the content-specific editing\\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\\nsimple yet effective method that reweights the delta values of different tokens\\nto modulate the focus of the model on the input image relative to the editing\\ninstruction, enabling continuous and fine-grained control over editing\\nintensity without any tuning. Extensive experiments conducted on existing image\\nediting frameworks demonstrate that GRAG can be integrated with as few as four\\nlines of code, consistently enhancing editing quality. Moreover, compared to\\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\\nprecise control over the degree of editing. Our code will be released at\\nhttps://github.com/little-misfit/GRAG-Image-Editing.\", 'published': datetime.datetime(2025, 10, 28, 17, 22, 44, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24657v1', 'categories': ['cs.CV']}, {'title': 'A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries', 'authors': ['Xin Zhang', 'Yuqi Song', 'Fei Zuo'], 'summary': \"The rapid advancement of generative AI has enabled the creation of highly\\nrealistic forged facial images, posing significant threats to AI security,\\ndigital media integrity, and public trust. Face forgery techniques, ranging\\nfrom face swapping and attribute editing to powerful diffusion-based image\\nsynthesis, are increasingly being used for malicious purposes such as\\nmisinformation, identity fraud, and defamation. This growing challenge\\nunderscores the urgent need for robust and generalizable face forgery detection\\nmethods as a critical component of AI security infrastructure. In this work, we\\npropose a novel dual-branch convolutional neural network for face forgery\\ndetection that leverages complementary cues from both spatial and frequency\\ndomains. The RGB branch captures semantic information, while the frequency\\nbranch focuses on high-frequency artifacts that are difficult for generative\\nmodels to suppress. A channel attention module is introduced to adaptively fuse\\nthese heterogeneous features, highlighting the most informative channels for\\nforgery discrimination. To guide the network's learning process, we design a\\nunified loss function, FSC Loss, that combines focal loss, supervised\\ncontrastive loss, and a frequency center margin loss to enhance class\\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\\nincludes forged images generated from four representative methods:\\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\\nstrong performance across all categories and outperforms average human\\naccuracy. These results demonstrate the model's effectiveness and its potential\\ncontribution to safeguarding AI ecosystems against visual forgery attacks.\", 'published': datetime.datetime(2025, 10, 28, 17, 6, 40, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24640v1', 'categories': ['cs.CV']}, {'title': 'All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level Spiking Neural Networks', 'authors': ['Andrea Castagnetti', 'Alain Pegatoquet', 'Beno√Æt Miramond'], 'summary': 'Spiking Neural Networks (SNNs) are one of the most promising bio-inspired\\nneural networks models and have drawn increasing attention in recent years. The\\nevent-driven communication mechanism of SNNs allows for sparse and\\ntheoretically low-power operations on dedicated neuromorphic hardware. However,\\nthe binary nature of instantaneous spikes also leads to considerable\\ninformation loss in SNNs, resulting in accuracy degradation. To address this\\nissue, we propose a multi-level spiking neuron model able to provide both\\nlow-quantization error and minimal inference latency while approaching the\\nperformance of full precision Artificial Neural Networks (ANNs). Experimental\\nresults with popular network architectures and datasets, show that multi-level\\nspiking neurons provide better information compression, allowing therefore a\\nreduction in latency without performance loss. When compared to binary SNNs on\\nimage classification scenarios, multi-level SNNs indeed allow reducing by 2 to\\n3 times the energy consumption depending on the number of quantization\\nintervals. On neuromorphic data, our approach allows us to drastically reduce\\nthe inference latency to 1 timestep, which corresponds to a compression factor\\nof 10 compared to previously published results. At the architectural level, we\\npropose a new residual architecture that we call Sparse-ResNet. Through a\\ncareful analysis of the spikes propagation in residual connections we highlight\\na spike avalanche effect, that affects most spiking residual architectures.\\nUsing our Sparse-ResNet architecture, we can provide state-of-the-art accuracy\\nresults in image classification while reducing by more than 20% the network\\nactivity compared to the previous spiking ResNets.', 'published': datetime.datetime(2025, 10, 28, 17, 3, 33, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24637v1', 'categories': ['cs.NE', 'cs.AI']}, {'title': 'Electrochemical Electron Transfer: Key Concepts, Theories, and Parameterization via Atomistic Simulations', 'authors': ['Mengke Zhang', 'Yanxia Chen', 'Marko M. Melander', 'Jun Huang'], 'summary': 'Electron transfer (ET) at electrochemical interfaces is central to energy\\nconversion and storage, yet its theoretical and computational modeling remain\\nactive research areas. This review elucidates key concepts and theories of ET\\nkinetics, focusing on coupling between classical solvent fluctuations and\\nquantum electronic states of metallic electrodes and redox species. We begin\\nwith fundamental rate theories, reaction coordinates, and electrochemical\\ntimescales, then explore weak, strong, and intermediate electronic coupling\\nregimes. Special attention is given to solvent dynamics and the structure of\\nthe electrical double layer (EDL), which critically impact ET kinetics.\\nAtomistic simulations, particularly density functional theory (DFT) and\\nmolecular dynamics (MD), are highlighted for testing linear response and\\ndetermining solvent reorganization energy, electronic coupling strengths, and\\nsolvent relaxation dynamics. A central theme is linear response enabling\\ntractable treatments across Marcus theory, empirical valence bond (EVB) models,\\nthe Anderson-Newns-Schmickler framework, and generalized Langevin dynamics.\\nWhile linear response offers useful simplifications, we assess its limitations,\\nparticularly for strong solvation changes or inner-sphere ET at catalytic\\ninterfaces. We discuss advances, including mapping Hamiltonian-based EVB-MD,\\nconstrained DFT, and non-Gaussian free energy formulations, enabling rigorous\\ntests and access to diabatic and adiabatic free energy surfaces. We outline\\nopportunities to advance multiscale, quantum-classical models that integrate\\nEDL effects, multiple reaction coordinates, solvent-controlled dynamics, and\\ntransitions between adiabatic and nonadiabatic regimes. This review serves as a\\nconceptual guide and practical resource for researchers integrating theory and\\nsimulation in studying electrochemical ET across diverse systems.', 'published': datetime.datetime(2025, 10, 28, 17, 2, 22, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24635v1', 'categories': ['physics.chem-ph']}, {'title': 'Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives', 'authors': ['Azadeh Pourkabirian', 'Amir Masoud Rahmani', 'Kai Li', 'Wei Ni'], 'summary': 'Delay-sensitive Internet of Things (IoT) applications have drawn significant\\nattention. Running many of these applications on IoT devices is challenging due\\nto the limited processing resources of these devices and the need for real-time\\nresponses. Task offloading can minimize latency by transferring computationally\\nintensive tasks from IoT devices to resource-rich edge servers, ensuring delay\\nand performance guarantees. In this paper, we develop a task-offloading\\napproach for delay-sensitive IoT applications in edge computing environments.\\nUnlike existing schemes, we model the task offloading problem as an economic\\ndemand and supply model to achieve market balance. The proposed model avoids\\nunder- and over-supply, ensuring the computational resources at edge servers\\n(supply) are allocated in a manner that best meets the processing and\\ncomputational needs of user devices (demand). Given the multi-agent nature of\\ntask offloading involving users and service providers with different\\npreferences and objectives, we design a game-theoretic framework using a\\nVickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions\\nand decision-making processes. Additionally, we develop an incentive mechanism\\nto encourage both parties to participate in the auction. The mechanism\\nmaximizes user task offloading to edge servers and motivates edge servers to\\nshare their computational resources, achieving profitability for both IoT users\\nand edge servers. Simulations demonstrate our method maximizes social welfare,\\nensures truthfulness, maintains market balance, and provides latency guarantees\\nfor delay-sensitive IoT applications.', 'published': datetime.datetime(2025, 10, 28, 16, 40, 33, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24611v1', 'categories': ['cs.NI']}, {'title': 'Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs', 'authors': ['Siheng Xiong', 'Joe Zou', 'Faramarz Fekri', 'Yae Jee Cho'], 'summary': 'The quadratic cost of attention hinders the scalability of long-context LLMs,\\nespecially in resource-constrained settings. Existing static sparse methods\\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\\nreduce the cost of attention, but poorly adapts to the content-dependent\\nvariations in attention due to their staticity. While previous work has\\nproposed several dynamic approaches to improve flexibility, they still depend\\non predefined templates or heuristic mechanisms. Such strategies reduce\\ngenerality and prune tokens that remain contextually important, limiting their\\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\\nonline without retraining. Our proposed DHSA adaptively segments sequences into\\nvariable-length chunks, then computes chunk representations by aggregating the\\ntoken embeddings within each chunk. To avoid the bias introduced by varying\\nchunk lengths, we apply length-normalized aggregation that scales the averaged\\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\\nchunk-level similarity scores to token level similarities to calculate\\nimportance scores that determine which token-level interactions should be\\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\\nLongBench show that DHSA matches dense attention in accuracy, while reducing\\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\\nrepresentative baselines such as block sparse attention, DHSA achieves\\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\\ncost, offering an efficient and adaptable solution for long-context on-device\\nLLMs.', 'published': datetime.datetime(2025, 10, 28, 16, 34, 18, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24606v1', 'categories': ['cs.CL']}, {'title': 'Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way', 'authors': ['Yicun Yang', 'Cong Wang', 'Shaobo Wang', 'Zichen Wen', 'Biqing Qi', 'Hanlin Xu', 'Linfeng Zhang'], 'summary': 'Diffusion-based large language models (dLLMs) have exhibited substantial\\npotential for parallel text generation, which may enable more efficient\\ngeneration compared to autoregressive models. However, current dLLMs suffer\\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\\nhave to be determined before decoding as a hyper-parameter, leading to issues\\nin efficiency and flexibility. To solve these problems, in this work, we\\npropose to train a diffusion LLM with native variable generation lengths,\\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\\npredict the [EOS] token in the generated text, which makes a dLLM be able to\\nnatively infer in a block diffusion manner, while still maintaining the ability\\nof global bi-directional (full) attention and high parallelism. Experiments on\\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\\nautoregressive models such as Qwen and Llama. Our method achieves higher\\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\\nsupporting their practical use in real-world applications. Codes and models\\nhave been released.', 'published': datetime.datetime(2025, 10, 28, 16, 32, 43, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24605v1', 'categories': ['cs.CL']}, {'title': 'Multifunctional Wideband Digital Metasurface for Secure Electromagnetic Manipulation in S-Band', 'authors': ['Longpan Wang', 'Zhuoran Zhang', 'Zhenyuan Li', 'Xuetao Gan', 'Xudong Bai', 'Wen Chen', 'Qingqing Wu'], 'summary': 'Digital metasurfaces have attracted significant attention in recent years due\\nto their ability to manipulate electromagnetic (EM) waves for secure sensing\\nand communication. However, most reported metasurfaces operate at relatively\\nhigh frequencies, primarily due to the constraints imposed by the physical\\nscale of the dielectric substrate, thus limiting their full-wave system\\napplications. In this work, a wideband digital reflective metasurface is\\npresented for capable of dynamically controlling EM waves, with multifunctional\\napplications in the lower-frequency S-band. The metasurface is composed of\\nelectronically reconfigurable meta-atoms with wideband characteristics, and\\ndesigned by using trapezoidal and M-shaped patches connected by a pin diode.\\nSimulation results show that the proposed digital metasurface could achieve\\nwideband 1-bit phase quantization with a stable phase difference within 180\\ndegree +/- 25 degree and small reflection loss below 0.6 dB from 2.72 to 3.25\\nGHz. To validate the proposed design, a 20x20-unit metasurface array was\\ndesigned, simulated and fabricated. By dynamically adjusting the coding\\nsequence, the metasurface could enable multi-mode orbital angular momentum\\n(OAM) beam generation, dynamic beam scanning, and precise direction finding.\\nThese capabilities support secure sensing and secure communications through\\nhigh-resolution target detection and anti-jamming beam steering, as well as\\nphysical-layer security. The proposed wideband metasurface may serve as an\\neffective candidate for enhancing spectral efficiency and security performance\\nin radar and wireless systems.', 'published': datetime.datetime(2025, 10, 28, 16, 26, 43, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24597v1', 'categories': ['eess.SP']}, {'title': 'LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis', 'authors': ['Qingyue Zhang', 'Chang Chu', 'Tianren Peng', 'Qi Li', 'Xiangyang Luo', 'Zhihao Jiang', 'Shao-Lun Huang'], 'summary': 'With the widespread adoption of LLMs, LoRA has become a dominant method for\\nPEFT, and its initialization methods have attracted increasing attention.\\nHowever, existing methods have notable limitations: many methods do not\\nincorporate target-domain data, while gradient-based methods exploit data only\\nat a shallow level by relying on one-step gradient decomposition, which remains\\nunsatisfactory due to the weak empirical performance of the one-step\\nfine-tuning model that serves as their basis, as well as the fact that these\\nmethods either lack a rigorous theoretical foundation or depend heavily on\\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\\nframework for data-aware LoRA initialization based on asymptotic analysis.\\nStarting from a general optimization objective that minimizes the expectation\\nof the parameter discrepancy between the fine-tuned and target models, we\\nderive an optimization problem with two components: a bias term, which is\\nrelated to the parameter distance between the fine-tuned and target models, and\\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\\na variance term, which accounts for the uncertainty introduced by sampling\\nstochasticity through the Fisher information. By solving this problem, we\\nobtain an optimal initialization strategy for LoRA. Building on this\\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\\nestimates the terms in the optimization problem from a small set of target\\ndomain samples and obtains the optimal LoRA initialization. Empirical results\\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\\naccuracy over existing initialization methods. Additional studies show faster,\\nmore stable convergence, robustness across ranks, and only a small\\ninitialization overhead for LoRA-DA. The source code will be released upon\\npublication.', 'published': datetime.datetime(2025, 10, 28, 15, 55, 36, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24561v1', 'categories': ['cs.LG', 'cs.AI']}]\n"
     ]
    }
   ],
   "source": [
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a98958-ded8-4676-a6bf-4e6da83f4853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
