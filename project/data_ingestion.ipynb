{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e9c1dd4-04c2-45fa-a6e2-1ef0cfa9b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "client = arxiv.Client()\n",
    "\n",
    "def fetch_arxiv_papers(query, max_results=10):  \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    papers = []\n",
    "    for result in search.results():\n",
    "        paper_info = {\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"summary\": result.summary,\n",
    "            \"published\": result.published,\n",
    "            \"url\": result.pdf_url,\n",
    "            \"categories\": result.categories\n",
    "        }\n",
    "        papers.append(paper_info)\n",
    "        return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d62cc6d-dad4-48c0-8742-212b9a1f0f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?', 'authors': ['Yihao Li', 'Saeed Salehi', 'Lyle Ungar', 'Konrad P. Kording'], 'summary': 'Object binding, the brain\\'s ability to bind the many features that\\ncollectively represent an object into a coherent whole, is central to human\\ncognition. It groups low-level perceptual features into high-level object\\nrepresentations, stores those objects efficiently and compositionally in\\nmemory, and supports human reasoning about individual object instances. While\\nprior work often imposes object-centric attention (e.g., Slot Attention)\\nexplicitly to probe these benefits, it remains unclear whether this ability\\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\\ncould: recognizing which patches belong to the same object should be useful for\\ndownstream prediction and thus guide attention. Motivated by the quadratic\\nnature of self-attention, we hypothesize that ViTs represent whether two\\npatches belong to the same object, a property we term IsSameObject. We decode\\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\\nin ImageNet-supervised models, suggesting that binding is not a trivial\\narchitectural artifact, but an ability acquired through specific pretraining\\nobjectives. We further discover that IsSameObject is encoded in a\\nlow-dimensional subspace on top of object features, and that this signal\\nactively guides attention. Ablating IsSameObject from model activations\\ndegrades downstream performance and works against the learning objective,\\nimplying that emergent object binding naturally serves the pretraining\\nobjective. Our findings challenge the view that ViTs lack object binding and\\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\\nnaturally in a connectionist system.', 'published': datetime.datetime(2025, 10, 28, 17, 57, 5, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/pdf/2510.24709v1', 'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'q-bio.NC']}]\n"
     ]
    }
   ],
   "source": [
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a98958-ded8-4676-a6bf-4e6da83f4853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/pdf/2510.24709v1\n"
     ]
    }
   ],
   "source": [
    "print(papers[0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4a55629-8011-4dc0-bbf7-5f86da254f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "def download_pdf_from_url(url,output_folder,file_name):\n",
    "    save_path = os.path.join(output_folder,file_name)\n",
    "    response = requests.get(url,stream =True)\n",
    "        \n",
    "    with open(save_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size= 6192):\n",
    "            f.write(chunk)    \n",
    "    return save_path\n",
    "    print(f\"PDF downloaded and saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b1aeeea-d41a-4175-a34a-deb3863a250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2513/358862634.py:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 papers\n"
     ]
    }
   ],
   "source": [
    "papers = fetch_arxiv_papers(query = 'attention', max_results= 10)\n",
    "output_folder = 'arxiv_papers'\n",
    "\n",
    "print(f\"Found {len(papers)} papers\") \n",
    "for idx,paper in enumerate(papers):\n",
    "    paper_id = paper['url'].split('/')[-1]\n",
    "    paper_url = paper['url']\n",
    "    file_name = f\"{paper_id}.pdf\"\n",
    "    download_pdf_from_url(url=paper_url,output_folder = output_folder,file_name=file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883aa07-6f7f-4f14-81d2-1aa852368ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
