{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d878358-694b-48d5-b157-b925fdfafba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "        \n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1b04275-58c8-4b4b-a637-8402594f271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_data():\n",
    "    repo_owner = 'evidentlyai'\n",
    "    repo_name = 'docs'\n",
    "\n",
    "    \n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions,\n",
    "      \n",
    "    )\n",
    "    \n",
    "    return reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ea1ebd-bfc3-40e3-91e0-94dc540bb22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = read_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea301ab-6583-4dcf-8f49-cf6907236f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Evidently and GitHub actions\"\n",
      "description: \"Testing LLM outputs as part of the CI/CD flow.\"\n",
      "---\n",
      "\n",
      "You can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application - as part of every code push or pull request.\n",
      "\n",
      "## How the integration work:\n",
      "\n",
      "- You define a test dataset of inputs (e.g. test prompts with or without reference answers). You can store it as a file, or save the dataset at Evidently Cloud callable by Dataset ID.\n",
      "- Run your LLM system or agent against those inputs inside CI.\n",
      "- Evidently automatically evaluates the outputs using the user-specified config (which defines the Evidently descriptors, tests and Report composition), including methods like:\n",
      "  - LLM judges (e.g., tone, helpfulness, correctness)\n",
      "  - Custom Python functions\n",
      "  - Dataset-level metrics like classification quality\n",
      "- If any test fails, the CI job fails.\n",
      "- You get a detailed test report with pass/fail status and metrics.\n",
      "\n",
      "![](/images/examples/github_actions.gif)\n",
      "\n",
      "Results are stored locally or pushed to Evidently Cloud for deeper review and tracking.\n",
      "\n",
      "The final result is CI-native testing for your LLM behavior - so you can safely tweak prompts, models, or logic without breaking things silently.\n",
      "\n",
      "## Code example and tutorial\n",
      "\n",
      "üëâ Check the full tutorial and example repo: https://github.com/evidentlyai/evidently-ci-example\n",
      "\n",
      "Action is also available on GitHub Marketplace: https://github.com/marketplace/actions/run-evidently-report\n"
     ]
    }
   ],
   "source": [
    "print(github_data[40].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a98615b-5bdd-4a74-839d-c58b4249b396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m122 packages\u001b[0m \u001b[2min 2.36s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 219ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m=1.1.0                            \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-frontmatter\u001b[0m\u001b[2m==1.1.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add python-frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b97768d-bbca-4a0f-8954-e0d27a8ce0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "\n",
    "def parse_data(data_raw):\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ead6684-d57a-4e86-a04b-8106a15da6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7fab43-28b7-44e3-9b2f-0612efb12fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Create Plant',\n",
       "  'openapi': 'POST /plants',\n",
       "  'content': '',\n",
       "  'filename': 'api-reference/endpoint/create.mdx'},\n",
       " {'title': 'Delete Plant',\n",
       "  'openapi': 'DELETE /plants/{id}',\n",
       "  'content': '',\n",
       "  'filename': 'api-reference/endpoint/delete.mdx'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'content': '',\n",
       "  'filename': 'api-reference/endpoint/get.mdx'},\n",
       " {'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'content': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       "  'filename': 'api-reference/introduction.mdx'},\n",
       " {'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'content': '<Update label=\"2025-07-18\" description=\"Evidently v0.7.11\">\\n  ## **Evidently 0.7.11**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).\\n\\nExample notebooks:\\n- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\\n\\n</Update>\\n\\n<Update label=\"2025-07-09\" description=\"Evidently v0.7.10\">\\n  ## **Evidently 0.7.10**\\n    Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).\\n  \\nNEW: automated prompt optimization. Read the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\\n\\nExample notebooks:\\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\\n</Update>\\n\\n<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\\n  ## **Evidently 0.7.9**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\\n</Update>\\n\\n<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.6).\\n</Update>\\n\\n<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\\n  ## **Evidently 0.7.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5).\\n</Update>\\n\\n<Update label=\"2025-05-05\" description=\"Evidently v0.7.4\">\\n  ## **Evidently 0.7.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4).\\n</Update>\\n\\n<Update label=\"2025-04-25\" description=\"Evidently v0.7.3\">\\n  ## **Evidently 0.7.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3).\\n</Update>\\n\\n<Update label=\"2025-04-22\" description=\"Evidently v0.7.2\">\\n  ## **Evidently 0.7.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2).\\n</Update>\\n\\n<Update label=\"2025-04-21\" description=\"Evidently v0.7.1\">\\n  ## **Evidently 0.7.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1).\\n</Update>\\n\\n\\n<Update label=\"2025-04-10\" description=\"Evidently v7.0\">\\n  ## **Evidently 0.7**\\n\\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2).\\n</Update>\\n\\n<Update label=\"2025-03-31\" description=\"Evidently v0.6.7\">\\n  ## **Evidently 0.6.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7).\\n</Update>\\n\\n<Update label=\"2025-03-12\" description=\"Evidently v0.6.6\">\\n  ## **Evidently 0.6.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.5\">\\n  ## **Evidently 0.6.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.4\">\\n  ## **Evidently 0.6.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4).\\n</Update>\\n\\n<Update label=\"2025-02-12\" description=\"Evidently v0.6.3\">\\n  ## **Evidently 0.6.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool).\\n</Update>\\n\\n<Update label=\"2025-02-07\" description=\"Evidently v0.6.2\">\\n  ## **Evidently 0.6.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.2). We extended support for `litellm` , so you can easily use different providers like Gemini, Anthropic, etc. for LLM-based evaluations.\\n</Update>\\n\\n<Update label=\"2025-01-31\" description=\"Evidently v0.6.1\">\\n  ## **Evidently 0.6.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.1).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently v0.6\">\\n  ## **New API release**\\n\\n  The new API is available when you import modules from `evidently.future`. Read more in [Migration guide](/faq/migration). Release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.0).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently Cloud\">\\n  ## **Editable datasets**\\n\\n  You can now hit \"edit\" on any existing dataset, create a copy and add / delete rows and columns. Use it while working on your evaluation datasets or to leave comments on outputs.\\n\\n  ![](/images/changelog/editable_dataset-min.png)\\n</Update>\\n\\n<Update label=\"2025-01-10\" description=\"Docs\">\\n  ## **New Docs**\\n\\n  We are creating a new Docs website in anticipation of API change. You can still access old docs for information on earlier API and examples.\\n</Update>',\n",
       "  'filename': 'changelog/changelog.mdx'},\n",
       " {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'content': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don‚Äôt need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/descriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they‚Äôll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping required for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<Info>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # If probabilistic classification\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional, for display only\\n    )]\\n)\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: str = \"prediction\"\\n    prediction_probas: Optional[List[str]] = None #if probabilistic classification\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n<Note>\\n  When you have multiclass classification with predicted probabilities in separate columns, the column names in `prediction_probas` must exactly match the class labels. For example, if your classes are 0, 1, and 2, your probability columns must be named: \"0\", \"1\", \"2\". Values in `target` and `prediction` columns should be strings.\\n</Note>\\n\\n#### Binary\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'title': 'Descriptors',\n",
       "  'description': 'How to run evaluations for text data.',\n",
       "  'content': 'To evaluate text data, like LLM inputs and outputs, you create **Descriptors**. This is a universal interface for all evals - from text statistics to LLM judges.\\n\\nEach descriptor computes a score or label per row of your dataset. You can combine multiple descriptors and set optional pass/fail conditions. You can use built-in descriptors or create custom ones using LLM prompts or Python.\\n\\nFor a general introduction, check [Core Concepts](/docs/library/overview). You can also refer to the [LLM quickstart](quickstart_llm) for a minimal example.\\n\\n## Basic flow\\n\\n<Accordion title=\"Generate toy data\" defaultOpen={false}>\\n  Use this code snippet to create sample data for testing:\\n\\n  ```python\\n  import pandas as pd\\n  \\n  data = [\\n      [\"What is the chemical symbol for gold?\", \"The chemical symbol for gold is Au.\"],\\n      [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n      [\"Tell me a joke.\", \"Why don\\'t programmers like nature? It has too many bugs!\"],\\n      [\"What is the boiling point of water?\", \"The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit).\"],\\n      [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n      [\"What‚Äôs the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n      [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework. You might want to consult your teacher for help.\"],\\n      [\"How many states are there in the USA?\", \"There are 50 states in the USA.\"],\\n      [\"What‚Äôs the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n      [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n  ]\\n  \\n  # Columns\\n  columns = [\"question\", \"answer\"]\\n  \\n  # Creating the DataFrame\\n  df = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\n\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals\\n```\\n\\n**Note**. Some Descriptors (like `OOVWordsPercentage()` may require `nltk` dictionaries:\\n\\n```python\\nnltk.download(\\'words\\')\\nnltk.download(\\'wordnet\\')\\nnltk.download(\\'omw-1.4\\')\\nnltk.download(\\'vader_lexicon\\')\\n```\\n\\n**Step 2. Add descriptors** via the Dataset object. There are two ways to do this:\\n\\n- **Option A.** Simultaneously create the `Dataset` object and add descriptors to the selected columns (in this case, \"answer\" column).\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(\\n        text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n    ]\\n)\\n```\\n<Info>\\nRead more on how how to [create the Dataset and Data Definition](/docs/library/data_definition)\\n</Info>\\n\\n- **Option B.** Add descriptors to the existing Dataset using `add_descriptors`.\\n\\nFor example, first create the Dataset.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n)\\n```\\n\\nThen, add the scores to this Dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Sentiment(\"answer\", alias=\"Sentiment\"),\\n    TextLength(\"answer\", alias=\"Length\"),\\n    IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n])\\n```\\n\\n**Step 3. (Optional). Export results**. You can preview the DataFrame with results: \\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/metrics/descriptors-min.png)\\n\\n**Step 4. Get the Report**. This will summarize the results, capturing stats and distributions for all descriptors. The easiest way to get the Report is through `TextEvals` Preset.\\n\\nTo configure and run the Report for the `eval_dataset`:\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\nmy_eval = report.run(eval_dataset)\\nmy_eval\\n\\n# my_eval.json()\\n# ws.add_report(project.id, my_eval, include_data=True)\\n```\\n\\nYou can view the Report in Python, export the outputs (HTML, JSON, Python dictionary) or upload it to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n\\n![](/images/metrics/descriptors-report.png)\\n\\n## Customizing descriptors\\n\\n<Tip>\\n  **All descriptors and parameters**. Evidently has multiple implemented descriptors, both deterministic and LLM-based. See a [reference table](/metrics/all_descriptors) with all descriptors and parameters.\\n</Tip>\\n\\n**Alias**. It is best to add an `alias` to each Descriptor to make it easier to reference. This name shows up in visualizations and column headers. It‚Äôs especially handy if you‚Äôre using checks like regular expressions with word lists, where the auto-generated title could get very long.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    WordCount(\"answer\", alias=\"Words\"),\\n])\\n```\\n\\n**Descriptor parameters**. Some Descriptors have required parameters. For example, if you‚Äôre testing for competitor mentions using the `Contains` Descriptor, add the list of `items`:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Contains(\"answer\", items=[\"AcmeCorp\", \"YetAnotherCorp\"], alias=\"Competitors\")\\n])\\n```\\n\\nThese parameters are specific to each descriptors. Check the [reference table](/metrics/all_descriptors).\\n\\n**Multi-column descriptors**. Some evals use more than one column. For example, to match a new answer against reference, or measure semantic similarity. Pass both columns using parameters:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    SemanticSimilarity(columns=[\"question\", \"answer\"], alias=\"Semantic_Match\")\\n])\\n```\\n\\n**LLM-as-a-judge**. There are also built-in descriptors that prompt an external LLM to return an evaluation score. You can add them like any other descriptor, but you must also provide an API key to use the corresponding LLM.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"answer\", alias=\"Contains_Denial\")\\n])\\n```\\n\\n<Tip>\\n  **Using and customizing LLM judge**. Check the [in-depth LLM judge guide](/metrics/customize_llm_judge) on using built-in and custom LLM-based evaluators.\\n</Tip>\\n\\n**Custom evals**. Beyond custom LLM judges, you can also implement your own programmatic evals as Python functions. Check the [custom descriptor guide](/metrics/customize_descriptor).\\n\\n## Adding Descriptor Tests\\n\\nDescriptor Tests let you define pass/fail checks for each row in your dataset. Instead of just calculating values (like ‚ÄúHow long is this text?‚Äù), you can ask:\\n\\n- Is the text under 100 characters?\\n- Is the sentiment positive?\\n\\nYou can also combine multiple tests into a single summary result per row.\\n\\n**Step 1. Imports**. Run imports:\\n\\n```python\\nfrom evidently.descriptors import ColumnTest, TestSummary\\nfrom evidently.tests import *\\n```\\n\\n**Step 2. Add tests to a descriptor**. When creating a descriptor (like `TextLength` or `Sentiment`), use the tests argument to set conditions. Each test adds a new column with a True/False result.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n    ]\\n)\\n```\\n\\nUse test parameters like `gte` (greater than or equal), `lte` (less than or equal), eq (equal). Check the [full list here](docs/library/tests#test-parameters).\\n\\nYou can preview the results with: `eval_dataset.as_dataframe()`:\\n![](/images/metrics/descriptors_tests-min.png)\\n\\n**Step 3. Add a Test Summary**. Use `TestSummary` to combine multiple tests into one or more summary columns. For example, the following returns True if all tests pass:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\", tests=[\\n            eq(\"OK\", column=\"Denials\", alias=\"Is not a refusal\")]),\\n        TestSummary(success_all=True, alias=\"Test result\"), #returns True if all conditions are satisfied\\n    ]\\n)\\n```\\n\\n<Info>\\n  `TestSummary` will only consider tests added **before it** in the list of descriptors.\\n</Info>\\n\\n<Info>\\nFor LLM judge descriptors returning multiple columns (e.g., label and reasoning), you must specify the target column for the test ‚Äî see `DeclineLLMEval` in the example.\\n</Info>\\n\\nYou can aggregate Test results differently and include multiple summary columns, such as total count, pass rate, or weighted score:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    TestSummary(\\n        success_all=True,     # True if all tests pass\\n        success_any=True,     # True if any test passes\\n        success_count=True,   # Total number of tests passed\\n        success_rate=True,    # Share of passed tests\\n        score=True,           # Weighted score\\n        score_weights={\\n            \"Sentiment is non-negative\": 0.9,\\n            \"Length is under 100\": 0.1,\\n        },\\n    )\\n])\\n```\\n\\n**Testing existing columns**. Use `ColumnTest` to apply checks to any column, even ones not generated by descriptors. This is useful for working with metadata or precomputed values:\\n\\n```python\\ndataset = Dataset.from_pandas(pd.DataFrame(data), descriptors=[\\n    ColumnTest(\"Feedback\", eq(\"Positive\")),\\n])\\n```\\n\\n## Summary Reports\\n\\nYou\\'ve already seen how to generate a report using the `TextEvals` preset. It\\'s the simplest and useful way to summarize evaluation results. However, you can also create custom reports using different metric combinations for more control.\\n\\n**Imports**. Import the components you\\'ll need:\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.metrics import *\\nfrom evidently.tests import *\\n```\\n\\n**Selecting a list of columns**. You can apply `TextEvals` to specific descriptors in your dataset. This makes your report more focused and lightweight.\\n\\n```python\\nreport = Report([\\n    TextEvals(columns=[\"Sentiment\", \"Length\", \"Test result\"])\\n])\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n**Custom Report with different Metrics**. Each Evidently Report is built from individual Metrics. For example, `TextEvals` internally uses `ValueStats` Metric for each descriptor. To customize the Report, you can reference specific descriptors and use metrics like `MeanValue`, `MaxValue`, etc:\\n\\n```python\\ncustom_report = Report([\\n    MeanValue(column=\"Length\"),\\n    MeanValue(column=\"Sentiment\")\\n])\\n\\nmy_custom_eval = custom_report.run(eval_dataset, None)\\nmy_custom_eval\\n```\\n\\n<Note>\\n  **List of all Metrics**. Check the [Reference table](/metrics/all_metrics). Consider using column-level Metrics like `MeanValue`, `MeanValue`, `MaxValue`, `QuantileValue`, `OutRangeValueCount` and `CategoryCount`.\\n</Note>\\n\\n**Drift detection**. You can also run advanced checks, like comparing distributions between two datasets, for example, to detect text length drift:\\n\\n```python\\ncustom_report = Report([\\n    ValueDrift(column=\"Length\"),\\n])\\n\\nmy_custom_eval = custom_report.run(eval_dataset, eval_dataset)\\nmy_custom_eval\\n```\\n\\n## Dataset-level Test Suites\\n\\nYou can also attach Tests to your Metrics to get pass/fail results at the **dataset** Report level. Example tests:\\n\\n- No response has sentiment \\\\< 0\\n- No response exceeds 150 characters\\n- No more than 10% of rows fail the summary test\\n\\n```python\\ntests = Report([\\n    MinValue(column=\"Sentiment\", tests=[gte(0)]),\\n    MaxValue(column=\"Length\", tests=[lte(150)]),\\n    CategoryCount(column=\"Test result\", category=False, share_tests=[lte(0.1)])\\n])\\n\\nmy_test_eval = tests.run(eval_dataset, None)\\nmy_test_eval\\n# my_test_eval.json()\\n```\\n\\nThis produces a Test Suite that shows clear pass/fail results for the overall dataset. This is useful for automated checks and regression testing.\\n\\n![](/images/metrics/descriptors-report-test.png)\\n\\n<Note>\\n  **Report and Tests API**. Check separate guides on [generating Reports](/docs/library/report) and setting [Test conditions](/docs/library/tests).\\n</Note>',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'title': 'Overview',\n",
       "  'description': 'End-to-end evaluation workflow.',\n",
       "  'content': 'This page shows the core eval workflow with the Evidently library and links to guides.\\n\\n## Define and run the eval\\n\\n<Tip>\\n  To log the evaluation results to the Evidently Platform, first connect to [Evidently Cloud](/docs/setup/cloud) or your [local workspace](/docs/setup/self-hosting) and [create a Project](/docs/platform/projects_manage). It\\'s optional: you can also run evals locally.\\n</Tip>\\n\\n<Steps>\\n  <Step title=\"Prepare the input data\">\\n    Get your data in a table like a `pandas.DataFrame`. More on [data requirements](/docs/library/overview#dataset). You can also [load data](/docs/platform/datasets_workflow) from Evidently Platform, like tracing or synthetic datasets.\\n  </Step>\\n\\n  <Step title=\"Create a Dataset object\">\\n    Create a Dataset object with `DataDefinition()` that specifies column role and types. You can also use default type detection. [How to set Data Definition](/docs/library/data_definition).\\n\\n    ```python\\n    eval_data = Dataset.from_pandas(\\n        source_df,\\n        data_definition=DataDefinition()\\n    )\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add descriptors\">\\n    For text evals, choose and compute row-level `descriptors`. Optionally, add row-level tests to get pass/fail for specific inputs. [How to use Descriptors](/docs/library/descriptors).\\n\\n    ```python\\n    eval_data.add_descriptors(descriptors=[\\n        TextLength(\"Question\", alias=\"Length\"),\\n        Sentiment(\"Answer\", alias=\"Sentiment\")\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"Configure Report\">\\n    For dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset()\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Test conditions\">\\n    Add dataset-level Pass/Fail conditions, like to check if all texts are in \\\\< 100 symbols length. How to [configure Tests](/docs/library/tests).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset(),\\n        MaxValue(column=\"Length\", tests=[lt(100)]),\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Tags and Timestamps\">\\n    Add `tags` or `metadata` to identify specific evaluation runs or datasets, or override the default `timestamp `. [How to add metadata](/docs/library/tags_metadata).\\n  </Step>\\n\\n  <Step title=\"Run the Report\">\\n    To execute the eval, `run`the Report on the `Dataset` (or two).\\n\\n    ```python\\n    my_eval = report.run(eval_data, None)\\n    ```\\n  </Step>\\n\\n  <Step title=\"Explore the results\">\\n    * To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).\\n\\n    ```python\\n    ws.add_run(project.id, my_eval, include_data=True)\\n    ```\\n\\n    * To view locally. [All output formats](/docs/library/output_formats).\\n\\n    ```python\\n    my_eval\\n    ##my_eval.json()\\n    ```\\n  </Step>\\n</Steps>\\n\\n## Quickstarts\\n\\nCheck for end-to-end examples:\\n\\n<CardGroup cols={2}>\\n  <Card title=\"LLM quickstart\" icon=\"comment-text\" href=\"/quickstart_llm\">\\n    Evaluate the quality of text outputs.\\n  </Card>\\n\\n  <Card title=\"ML quickstart\" icon=\"table\" href=\"/quickstart_ml\">\\n    Test tabular data quality and data drift.\\n  </Card>\\n</CardGroup>',\n",
       "  'filename': 'docs/library/evaluations_overview.mdx'},\n",
       " {'title': 'Leftovers',\n",
       "  'description': 'Description of your new file.',\n",
       "  'noindex': True,\n",
       "  'content': \"Use for relevant pages after features are implemented.\\n\\n# Metrics\\n\\n## Correlations\\n\\nUse for exploratory data analysis, drift monitoring (correlation changes) or to check alignment between scores (e.g. LLM-based descriptors against human labels).\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map column types.\\n</Info>\\n\\nColumn data quality\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------\\n| **RowsWithMissingValuesCount()**  (Coming soon) | <ul><li> Dataset-level.</li><li>Counts rows with missing values.</li><li>Metric result: `value`.</li></ul>                                                                             | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one row with missing values.</li><li>**With reference**: Fails if share differs by >10% (+/-)</li></ul>           |\\n| **AlmostEmptyColumnCount()**  (Coming soon)     | <ul><li> Dataset-level.</li><li>Counts almost empty columns (95% empty).</li><li>Metric result: `value`.</li></ul>                                                                     | **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                          | <ul><li>**No reference**: Fails if there is at least one almost empty column.</li><li>**With reference**: Fails if count is higher than in reference.</li></ul>        |\\n| **NewCategoriesCount()** (Coming soon)                                                                                         | <ul><li>Column-level.</li><li>Counts new categories compared to reference (reference required).</li><li>Metric result: `count`, `share`.</li></ul>   | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |\\n| **MissingCategoriesCount()**  (Coming soon)                                                                                    | <ul><li>Column-level.</li><li>Counts missing categories compared to reference.</li><li>Metric result: `count`, `share`.</li></ul>                    | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | Expect 0.                                                                                                                                                          |\\n| **MostCommonValueCount()** (Coming soon)                                                                                       | <ul><li>Column-level.</li><li>Identifies the most common value and provides its count/share.</li><li>Metric result: `value: count, share`.</li></ul> | **Required**: <ul><li>`column`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                | <ul><li>**No reference**: Fails if most common value share is ‚â•80%.</li><li>**With reference**:  Fails if most common value share differs by >10% (+/-).</li></ul> |\\n\\nDrift\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- \\n| **EmbeddingDrift()** (Coming soon)    | <ul><li>Column-level.</li><li> Requires reference.</li><li>Calculates data drift for embeddings.</li><li>Requires embedding columns set in data definition.</li><li>Metric result: `value`.</li></ul>                                                                                                                                      | **Required**: <ul><li>`embeddings`</li><li>`method`</li></ul> See [embedding drift options](/metrics/customize_embedding_drift).                                                                                                                                                       | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_embedding_drift).</li></ul>                            |\\n| **MultivariateDrift()** (Coming soon) | <ul><li>Dataset-level.</li><li> Requires reference.</li><li>Computes a single dataset drift score.</li><li>Default method: share of drifted columns.</li><li>Metric result: `value`.</li></ul>                                                                                                                                             | **Optional**: <ul><li>`columns`</li><li>`method`</li></ul>See [drift options](/metrics/customize_data_drift).                                                                                                                                                                          | <ul><li>**With reference**: Defaults for method. See [methods](/metrics/customize_data_drift).             </li></ul>                    |\\n\\n| Metric                                  | Description                                                                                                                                                                                               | Parameters                                                                                                                                                                                                                        | Test Defaults                                                                                  |\\n| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\\n| **DatasetCorrelations()** (Coming soon) | <ul><li>Calculates the correlations between all or set columns in the dataset.</li><li>Supported methods: Pearson, Spearman, Kendall, Cramer\\\\_V.</li></ul>                                                | **Optional**: <ul><li>`columns`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                                                          | N/A                                                                                            |\\n| **Correlation()** (Coming soon)         | <ul><li>Calculates the correlation between two defined columns.</li></ul>                                                                                                                                 | **Required**: <ul><li>`column_x`</li><li>`column_y`</li></ul>**Optional**:<ul><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>[Test conditions](/docs/library/tests)</li></ul> | N/A                                                                                            |\\n| **CorrelationChanges()** (Coming soon)  | <ul><li>Dataset-level.</li><li>Reference required.</li><li>Checks the number of correlation violations (significant changes in correlation strength between columns) across all or set columns.</li></ul> | **Optional**: <ul><li>`columns`</li><li>`method` (default: `pearson`, available: `pearson`, `spearman`, `kendall`, `cramer_v`)</li><li>`corr_diff` (default: 0.25)</li><li>[Test conditions](/docs/library/tests)</li></ul>       | <ul><li>**With reference**: Fails if at least one correlation violation is detected.</li></ul> |\\n\\n\\nClassification\\n\\n| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **LabelCount()** (Coming soon) | <ul><li>Distribution of predicted classes.</li><li>Can visualize class balance and/or probability distribution.</li></ul>                                                   | **Required**: <ul><li>Set at least one visualization: `class_balance`, `prob_distribution`.</li></ul>  **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                                                                    | N/A                                                                                                                                                                                  |\\n| **Lift()**  (Coming soon)      | <ul><li>Calculates lift.</li><li>Can visualize lift curve or table.</li><li>Metric result: `value`.</li></ul>                                                               | **Required**: <ul><li>Set at least one visualization: `lift_table`, `lift_curve`.</li></ul> **Optional**: <ul><li>`probas_threshold`</li><li>`top_k`</li><li>[Test conditions](/docs/library/tests)</li></ul>                                                                    | N/A                                                                                                                                                                                  |\\n\\nRecsys\\n\\n<Info>\\n  [Data definition](/docs/library/data_definition). You may need to map prediction and target columns and ranking type. Some metrics require additional training data.\\n</Info>\\n\\n| Metric                              | Description                                                                                                                                                                         | Parameters                                                                                                                                                     | Test Defaults                                                                                                                           |\\n| ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\\n| **RecSysPreset()**                  | <ul><li>Larget Preset. </li><li>Includes a range of recommendation system metrics.</li><li>Metric result: all metrics.</li><li>See [Preset page](/metrics/preset_recsys).</li></ul> | None.                                                                                                                                                          | As in individual metrics.                                                                                                               |\\n| **Personalization()** (Coming soon) | <ul><li>Calculates Personalization score at the top K recommendations.</li><li>Metric result: `value`.</li></ul>                                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Personalization > 0.</li><li>**With reference**: Fails if Personalization differs by >10%.</li></ul> |\\n| **ARP()** (Coming soon)             | <ul><li>Computes Average Recommendation Popularity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`normalize_arp` (default: `False`)</li><li>[Test conditions](/docs/library/tests)</li></ul>          | <ul><li>**No reference**: Tests if ARP > 0.</li><li>**With reference**: Fails if ARP differs by >10%.</li></ul>                         |\\n| **Coverage()**(Coming soon)         | <ul><li>Calculates Coverage at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                            | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Coverage > 0.</li><li>**With reference**: Fails if Coverage differs by >10%.</li></ul>               |\\n| **GiniIndex()**(Coming soon)        | <ul><li>Calculates Gini Index at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                          | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Gini Index \\\\< 1.</li><li>**With reference**: Fails if Gini Index differs by >10%.</li></ul>          |\\n| **Diversity()**  (Coming soon)      | <ul><li>Calculates Diversity at the top K recommendations.</li><li>Requires item features.</li><li>Metric result: `value`.</li></ul>                                                | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                             | <ul><li>**No reference**: Tests if Diversity > 0.</li><li>**With reference**: Fails if Diversity differs by >10%.</li></ul>             |\\n| **Serendipity()**(Coming soon)      | <ul><li>Calculates Serendipity at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                         | **Required**: <ul><li>`k`</li><li>`item_features`</li></ul> **Optional**: <ul><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul>     | <ul><li>**No reference**: Tests if Serendipity > 0.</li><li>**With reference**: Fails if Serendipity differs by >10%.</li></ul>         |\\n| **Novelty()**  (Coming soon)        | <ul><li>Calculates Novelty at the top K recommendations.</li><li>Requires a training dataset.</li><li>Metric result: `value`.</li></ul>                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>[Test conditions](/docs/library/tests)</li></ul>                                                     | <ul><li>**No reference**: Tests if Novelty > 0.</li><li>**With reference**: Fails if Novelty differs by >10%.</li></ul>                 |\\n\\nRelevant for RecSys metrics:\\n\\n* `no_feedback_user: bool = False`. Specifies whether to include the users who did not select any of the items, when computing the quality metric. Default: False.\\n\\n* `min_rel_score: Optional[int] = None`. Specifies the minimum relevance score to consider relevant when calculating the quality metrics for non-binary targets (e.g., if a target is a rating or a custom score).\\n\\n# Ranking metrics explainers\\n\\n### Diversity\\n\\n**Evidently Metric**: `Diversity`\\n\\n**Recommendation diversity**: this metric measures the average intra-list diversity at K. It reflects the variety of items within the same user's recommendation list, averaged by all users. \\n\\n**Implemented method**:\\n* **Measure the difference between recommended items**. Calculate the Cosine distance for each pair of recommendations inside the top-K in each user's list. The cosine distance serves as a measure of diversity between vectors representing recommended items, and is computed as:\\n\\n$$\\\\text{Cosine distance} = 1 - \\\\text{Cosine Similarity}$$\\n\\nLink: [Cosine Similarity on Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity). \\n\\n* **Intra-list diversity**. Calculate intra-list diversity for each user by averaging the Cosine Distance between each pair of items in the user's top-K list.\\n* **Overall diversity**. Calculate the overall diversity by averaging the intra-list diversity across all users.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n**0:** identical recommendations in top-K.\\n**2:** very diverse recommendations in top-K.\\n\\n**Interpretation**: the higher the value, the more varied items are shown to each user (e.g. inside a single recommendation block).\\n\\n**Requirements**: You must pass the `item_features` list to point to numerical columns or embeddings that describe the recommended items. For example, these could be encoded genres that represent each movie. This makes it possible to compare the degree of similarity between different items. \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender system showing varied but irrelevant items will have high diversity.\\n* This method performs many pairwise calculations between items and can take some time to compute.\\n  \\n### Novelty\\n\\n**Evidently Metric**: `Novelty`\\n\\n**Recommendation novelty**: this metric measures the average novelty of recommendations at K. It reflects how unusual top-K items are shown to each user, averaged by all users. \\n\\n**Implemented method**:\\n* Measure **novelty of recommended items**. The novelty of an item can be defined based on its popularity in the training set.\\n\\n$$\\\\text{novelty}_i = -\\\\log_2(p_i)$$\\nwhere *p* represents the probability that item *i* is observed. It is calculated as the share of users that interacted with an item in the training set.\\n\\n$$\\\\text{novelty}_i = -\\\\log_2\\\\left(\\\\frac{\\\\text{users who interacted with } i}{\\\\text{number of users}}\\\\right)$$\\n\\nHigh novelty corresponds to long-tail items that few users interacted with, and low novelty values correspond to popular items. If all users had interacted with an item, novelty is 0.\\n* Measure **novelty by user**. For each user, compute the average item novelty at K, by summing up the novelty of all items and dividing by K.\\n* **Overall novelty**. Average the novelty by user across all users.\\n\\n**Range**: 0 to infinity. \\n\\n**Interpretation**: if the value is higher, the items shown to users are more unusual. If the value is lower, the recommended items are well-known.   \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender system showing many irrelevant but unexpected (long tail) items will have high novelty. \\n* It is not possible to define the novelty of an item absent in the training set. The evaluation only considers items that are present in training. \\n\\nFurther reading: [Castells, P., Vargas, S., & Wang, J. (2011). Novelty and Diversity Metrics for Recommender Systems: Choice, Discovery and Relevance](https://repositorio.uam.es/bitstream/handle/10486/666094/novelty_castells_DDR_2011.pdf)\\n\\n### Serendipity\\n\\n**Evidently Metric**: `Serendipity`\\n\\nRecommendation serendipity: this metric measures how unusual the relevant recommendations are in K, averaged for all users. \\n\\nSerendipity combines unexpectedness and relevance. It reflects the ability of a recommender system to show relevant items (that get a positive ranking or action) that are unexpected in the context of the user history (= are not similar to previous interactions). For example, a user who usually likes comedies gets recommended and upvotes a thriller.\\n\\n**Implemented method**. \\n* Measure the **unexpectedness** of relevant recommendations. The ‚Äúunexpectedness‚Äù is measured using Cosine distance. For every relevant recommendation in top-K, we compute the distance between this item and the previous user interactions in the training set. Higher cosine distance indicates higher unexpectedness.\\n\\n$$\\\\text{serendipity}_i = \\\\text{unexpectedness}_i\\\\times\\\\text{relevance}_i$$\\n\\nWhere *relevance(i)* is equal to 1 if the item is relevant, and is 0 otherwise.\\n* **Serendipity by user**. Calculate the average of the resulting distances for all relevant recommendations in the user list.  \\n* **Overall serendipity**. Calculate the overall recommendation serendipity by averaging the results across all users.\\n\\n$$\\\\text{Serendipity} = 1 - \\\\sum_{u \\\\in S} \\\\frac{1}{|S| |H_u|} \\\\sum_{h \\\\in H_u} \\\\sum_{i \\\\in R_{u,k}} \\\\frac{\\\\text{CosSim}(i, h)}{k}$$\\n\\nWhere\\n* *S* is the set of all users.\\n* *H(u)* is the item history of user *u*.\\n* *R(u)* Top-K function, where *R(u,k)* gives the top *k* recommended items for user *u*.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n* **0**: only popular, expected relevant recommendations.\\n* **2**: completely unexpected relevant recommendations.\\n \\n**Interpretation**: the higher the value, the better the ability of the system to ‚Äúpositively surprise‚Äù the user. \\n\\n**Requirements**: You must pass the `item_features` list to point to the numerical columns or embeddings that describe the recommended items. This allows comparing the degree of similarity between recommended items.\\n\\n**Notes**: \\n* This metric is only computed for the users that are present in the training set. If there is no previous recommendation history, these users will be ignored. \\n* This metric only considers the unexpectedness of relevant items in top-K. Irrelevant recommendations, and their share, are not taken into account.\\n\\nFurther reading: [Zhang, Y., S√©aghdha, D., Quercia, D., Jambor, T. (2011). Auralist: introducing serendipity into music recommendation.](http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf)\\n\\n### Personalization\\n\\n**Evidently Metric**: `Personalization`\\n\\nPersonalization of recommendations: this metric measures the average uniqueness of each user's recommendations in top-K.\\n\\n**Implemented method**:\\n* For every two users, compute the **overlap between top-K recommended items**. (The number of common items in top-K between two lists, divided by K).\\n* Calculate the **average overlap** across all pairs of users.\\n* Calculate personalization as: \\n\\n$$\\\\text{Personalization} = 1 - \\\\text{average overlap}$$\\n\\nThe resulting metric reflects the average share of unique recommendations in each user‚Äôs list.\\n\\n**Range**: 0 to 1.\\n* **0**: Identical recommendations for each user in top-K. \\n* **1**: Each user‚Äôs recommendations in top-K are unique.   \\n\\n**Interpretation**: the higher the value, the more personalized (= different from others) is each user‚Äôs list. The metric visualization also shows the top-10 most popular items.\\n\\n### Average Recommendation Popularity (ARP)\\n\\n**Evidently Metric**: `ARP`\\n\\nThe recommendation popularity bias is a tendency to favor a few popular items. \\n\\nARP reflects the average popularity of the items recommended to the users. \\n\\n**Implementation**.\\n* Compute the item popularity as the number of times each item was seen in training. \\n* Compute the average popularity for each user‚Äôs list as a sum of all items‚Äô popularity divided by the number of recommended items.\\n* Compute the average popularity for all users by averaging the results across all users.\\n\\n$$ARP = \\\\frac{1}{|U|} \\\\sum_{u \\\\in U} \\\\frac{1}{|L_u|} \\\\sum_{i \\\\in L_u} \\\\phi(i)$$\\n\\nWhere:\\n* *U* is the total number of users.\\n* *L(u)* is the list of items recommended for the user *u*.\\n* *œï(i)* is the number of times item *i* was rated in the training set (popularity of item *i*)\\n\\n**Range**: 0 to infinity \\n\\n**Interpretation**: the higher the value, the more popular on average the recommendations are in top-K.  \\n\\n**Note**: This metric is not normalized and depends on the number of recommendations in the training set.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)\\n\\n### Coverage\\n\\n**Evidently Metric**: `Coverage`\\n\\nCoverage reflects the item coverage as a proportion of items that has been recommended by the system.\\n\\n**Implementation**: compute the share of items recommended to the users out of the total number of potential items (as seen in the training dataset).\\n\\n$$\\\\text{Coverage} = \\\\frac{\\\\text{Number of unique items recommended} K}{\\\\text{Total number of unique items}}$$\\n\\n**Range**: 0 to 1, where 1 means that 100% of items have been recommended to users. \\n\\n**Interpretation**: the higher the value (usually preferable), the larger the share of items represented in the recommendations. Popularity-based recommenders that only recommend a limited number of popular items will have low coverage.\\n\\n### Gini index \\n\\n**Evidently Metric**: `GiniIndex`\\n\\nGini index: reflects the inequality in the distribution of recommended items shown to different users, as compared to a perfectly equal distribution. \\n\\n**Implementation**:  \\n\\n$$ Gini(L) = 1 - \\\\frac{1}{|I| - 1} \\\\sum_{k=1}^{|I|} (2k - |I| - 1) p(i_k | L)$$\\n\\nWhere \\n* *L* is the combined list of all recommendation lists given to different users (note that an item may appear multiple times in L, if recommended for more than one user).\\n* *p(i|L)* is the ratio of occurrence of item *i* in *L*.\\n* *I* is the set of all items in the catalog.\\n\\n**Range**: 0 to 1, where 0 represents the perfect equality (recommended items are evenly distributed among users), and 1 is complete inequality (the recommendations are concentrated on a single item).\\n\\n**Interpretation**: the lower the value (usually preferable), the more equal the item distribution in recommendations. If the value is high, a few items are frequently recommended to many users while others are ignored.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)\",\n",
       "  'filename': 'docs/library/leftover_content.mdx'},\n",
       " {'title': 'Metric generators',\n",
       "  'description': 'How to generate multiple metrics at once.',\n",
       "  'content': 'Sometimes you need to generate multiple column-level Tests or Metrics. To simplify this, you can use metric generator helper functions.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Imports\\n\\n<Accordion title=\"Generate data\" defaultOpen={false}>\\nUse the following code to generate toy data for this guide.\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n\\nnp.random.seed(42)\\n\\ndata = {\\n    \"Age\": np.random.randint(18, 60, size=30),\\n    \"Salary\": np.random.randint(30000, 120000, size=30),\\n    \"Department\": np.random.choice([\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"], size=30),\\n    \"YearsExperience\": np.random.randint(1, 15, size=30),  \\n    \"EducationLevel\": np.random.choice([\"High School\", \"Bachelor\", \"Master\", \"PhD\"], size=30)  \\n}\\n\\ndummy_df = pd.DataFrame(data)\\n\\neval_data_1 = Dataset.from_pandas(\\n    dummy_df.iloc[:15],\\n    data_definition=DataDefinition()\\n)\\neval_data_2 = Dataset.from_pandas(\\n    dummy_df.iloc[15:],\\n    data_definition=DataDefinition()\\n)\\n```\\n</Accordion>\\n\\nImports\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.generators import ColumnMetricGenerator\\n```\\n\\n## Metric generators\\n\\n**Example 1**. Apply the selected metric (`ValueDrift`) to all columns in the dataset. \\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(ValueDrift)\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\n**Example 2**. Apply the selected metric (`ValueDrift`) to the listed columns in the dataset. Use `metric_kwargs` to pass any applicable metric parameters.\\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(ValueDrift, \\n                          columns=[\"EducationLevel\", \"Salary\"],\\n                          metric_kwargs={\"method\":\"psi\"}), # metric parameters\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\n**Example 3**. Apply the selected metric (`ValueDrift`) only to the categorical (`cat`) columns in the dataset.\\n\\n```python\\nreport = Report([\\n    ColumnMetricGenerator(UniqueValueCount, \\n                          column_types=\\'cat\\'),  #apply to categorical columns only \\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\nAvailable: \\n* `num` - numerical\\n* `cat` - categorical\\n* `all` - all \\n\\n## Test generators\\n\\nYou can use the same approach to generate Tests. Use `metric_kwargs` to pass test conditions.\\n\\n**Example.** Generate the same Test for all the columns in the dataset. It will use defaults if you do not specify the test condition.\\n\\n```python\\nfrom evidently.future.tests import *\\n\\nreport = Report([\\n    ColumnMetricGenerator(MinValue, \\n                          column_types=\\'num\\',\\n                          metric_kwargs={\"tests\":[gt(0)]}), \\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n```\\n\\nThis will apply the minimum value test to all numerical columns in the dataset and check that they are above 0.',\n",
       "  'filename': 'docs/library/metric_generator.mdx'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6d5b535-4fa5-4592-b89e-57004811cd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[3]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b840b6dc-934b-4bed-b150-9ba55af7619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97bf9e70-a173-4d51-8214-2d2e2b5f2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd74a89c-36b0-47db-bb85-dfbf47fa587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b0674e8-8062-4d02-afb4-2a721f82c185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 2000,\n",
       " 'content': 'eleases/tag/v0.7.6).\\n</Update>\\n\\n<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\\n  ## **Evidently 0.7.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5).\\n</Update>\\n\\n<Update label=\"2025-05-05\" description=\"Evidently v0.7.4\">\\n  ## **Evidently 0.7.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4).\\n</Update>\\n\\n<Update label=\"2025-04-25\" description=\"Evidently v0.7.3\">\\n  ## **Evidently 0.7.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3).\\n</Update>\\n\\n<Update label=\"2025-04-22\" description=\"Evidently v0.7.2\">\\n  ## **Evidently 0.7.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2).\\n</Update>\\n\\n<Update label=\"2025-04-21\" description=\"Evidently v0.7.1\">\\n  ## **Evidently 0.7.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1).\\n</Update>\\n\\n\\n<Update label=\"2025-04-10\" description=\"Evidently v7.0\">\\n  ## **Evidently 0.7**\\n\\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2).\\n</Update>\\n\\n<Update label=\"2025-03-31\" description=\"Evidently v0.6.7\">\\n  ## **Evidently 0.6.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7).\\n</Update>\\n\\n<Update label=\"2025-03-12\" description=\"Evidently v0.6.6\">\\n  ## **Evidently 0.6.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.5\">\\n  ## **Evidently 0.6.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5).\\n</Update>\\n\\n<Update label=\"2025-0',\n",
       " 'title': 'Product updates',\n",
       " 'description': 'Latest releases.',\n",
       " 'filename': 'changelog/changelog.mdx'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2f84381-0091-43b6-901c-3dd94174d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daf5712c-4506-447e-b0c6-165fa4327f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x78524c9e52e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d504a141-d441-4d27-b28f-cca71d948231",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search('how do I use llm-as-a-judge for evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d4a300f-bbc8-41b1-b4ff-9f8aa934943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9893878e-3003-43ab-9606-7abde3b2baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how do I use llm-as-a-judge for evals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e779204-9941-4c4b-8d57-cd87fa8b769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a4c29eb-7db3-44b6-aec4-edcc95570694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b7965bf-01f0-4434-9c98-303a710d9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2bace39-5cc2-4446-ac94-3e624fa27a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6532bfc8-9b8b-4d6b-8ca5-90e69293d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag('How can I build an eval report with llm as a judge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45a36884-aa29-41b2-ad63-230462fb5f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build an evaluation report with an LLM (Large Language Model) as a judge, you can follow these steps:\n",
      "\n",
      "### 1. **Installation and Imports**\n",
      "First, ensure you have the `evidently` library installed:\n",
      "\n",
      "```bash\n",
      "pip install evidently\n",
      "```\n",
      "\n",
      "Then, import the necessary modules:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "from evidently import Dataset, DataDefinition, Report, BinaryClassification\n",
      "from evidently.presets import TextEvals\n",
      "from evidently.metrics import *\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "```\n",
      "\n",
      "### 2. **Set Up Your OpenAI Key**\n",
      "Before using the LLM, set your OpenAI API key in your environment:\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "### 3. **Create the Evaluation Dataset**\n",
      "Create a dataset with your specific evaluations, including questions, approved answers, new responses, and manual labels:\n",
      "\n",
      "```python\n",
      "data = [\n",
      "    [\"Question 1?\", \"Approved Answer 1\", \"New Response 1\", \"incorrect\", \"Reason 1\"],\n",
      "    [\"Question 2?\", \"Approved Answer 2\", \"New Response 2\", \"correct\", \"Reason 2\"],\n",
      "    # Add more items as needed\n",
      "]\n",
      "\n",
      "golden_dataset = pd.DataFrame(data, columns=[\"question\", \"target_response\", \"new_response\", \"label\", \"explanation\"])\n",
      "```\n",
      "\n",
      "### 4. **Define the Data Structure**\n",
      "Map the column types for your dataset:\n",
      "\n",
      "```python\n",
      "definition = DataDefinition(\n",
      "    text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
      "    categorical_columns=[\"label\"]\n",
      ")\n",
      "\n",
      "eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
      "```\n",
      "\n",
      "### 5. **Set Up the LLM Judge**\n",
      "Define the prompt template for the LLM evaluator (e.g., for correctness evaluation):\n",
      "\n",
      "```python\n",
      "correctness = BinaryClassificationPromptTemplate(\n",
      "    criteria=\"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details.\n",
      "    The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, or omits details.\n",
      "    REFERENCE: {target_response}\"\"\",\n",
      "    target_category=\"correct\",\n",
      "    non_target_category=\"incorrect\",\n",
      "    uncertainty=\"unknown\",\n",
      "    include_reasoning=True,\n",
      "    pre_messages=[(\"system\", \"You are an expert evaluator.\")]\n",
      ")\n",
      "\n",
      "eval_dataset.add_descriptors(descriptors=[\n",
      "    LLMEval(\"new_response\",\n",
      "            template=correctness,\n",
      "            provider=\"openai\",\n",
      "            model=\"gpt-4o-mini\",\n",
      "            alias=\"Correctness\",\n",
      "            additional_columns={\"target_response\": \"target_response\"})\n",
      "])\n",
      "```\n",
      "\n",
      "### 6. **Run the Evaluation Report**\n",
      "Create and run the report using Evidently:\n",
      "\n",
      "```python\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "```\n",
      "\n",
      "### 7. **View and Analyze the Results**\n",
      "You can view the results and summaries:\n",
      "\n",
      "```python\n",
      "eval_dataset.as_dataframe()\n",
      "```\n",
      "\n",
      "This will show a DataFrame with newly added scores and explanations. You can generate an HTML report as well:\n",
      "\n",
      "```python\n",
      "report.run(eval_dataset, None).show()\n",
      "```\n",
      "\n",
      "### 8. **Evaluate the LLM‚Äôs Performance**\n",
      "To evaluate how well the LLM is performing, set up a classification metric on the results:\n",
      "\n",
      "```python\n",
      "class_definition = DataDefinition(\n",
      "    classification=[BinaryClassification(\n",
      "        target=\"label\",\n",
      "        prediction_labels=\"Correctness\",\n",
      "        pos_label=\"incorrect\")],\n",
      "    categorical_columns=[\"label\", \"Correctness\"]\n",
      ")\n",
      "\n",
      "class_dataset = Dataset.from_pandas(\n",
      "    eval_dataset.as_dataframe(),\n",
      "    data_definition=class_definition\n",
      ")\n",
      "\n",
      "classification_report = Report([ClassificationPreset()])\n",
      "my_eval_classification = classification_report.run(class_dataset, None)\n",
      "```\n",
      "\n",
      "### 9. **Iterate and Improve**\n",
      "Fine-tune your LLM‚Äôs prompts based on the results and iterate on your evaluation process as needed.\n",
      "\n",
      "This process enables you to effectively use an LLM as a judge for evaluating responses against expected outputs, leading to better insights and improvements in your evaluation workflows.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2be15-d603-4313-ae6e-d23efd42d886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
