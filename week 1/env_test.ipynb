{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00e3dcf-18ac-43e2-bb09-a4a42c3ff4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18da626-df0b-40bd-8305-5025959a2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "693873db-a1f1-4eb5-9deb-8d69c59be5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"Attention Is All You Need\" paper introduces the Transformer model, which relies entirely on self-attention mechanisms to process sequences of data, eliminating the need for recurrent or convolutional layers. This architecture enables more efficient computation and improved parallelization, leading to significant advancements in natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "response = openai_client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = \"explain attention is all you need article in 2 sentences\"\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ff7de-502f-4a76-9536-9566fe88274e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
